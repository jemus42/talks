---
title: "Variable Selection for Competing Risks with Cooperative Penalized Regression"
subtitle: "«CooPeR»"
author: 
  - Lukas Burk\inst{1,2,3,4}
  - Andreas Bender\inst{2,4}
  - Marvin N. Wright\inst{1,4}
format: 
  beamer:
    template: template.tex
    widescreen: false
    occasion: "CEN 2023"
    email: burk@leibniz-bips.de
    themeoptions: fira
    thankstext: Thank you for your attention!
    contactauthor: Lukas Burk
    institute:
      # Can't use \inst because title frame is made with tikz and \inst doesn't work within tikz nodes
      # and \insertinstitute did not work for some other godforsaken reason, idunno
        - \textsuperscript{1}Leibniz Institute for Prevention Research and Epidemiology -- BIPS
        - \textsuperscript{2}LMU Munich
        - \textsuperscript{3}University of Bremen
        - \textsuperscript{4}Munich Center for Machine Learning
    bibliography: references.bib
    cite-method: biblatex
    keep-tex: true
    # nocite: |
    #   @tay2023featureweightedelastic
---

```{r setup, include=FALSE, eval=FALSE}
library(ggplot2)
library(dplyr)
results <- readRDS("varsel-sim-results.rds")

theme_set(
  theme_minimal(base_size = 16) +
  theme(
    legend.position = "bottom", 
    panel.spacing = unit(10, "mm"), 
    plot.title.position = "plot"
  )
)
```


## Introduction

- Setting: High-dimensional survival data + competing risks, e.g
    - (1) death from bladder cancer
    - (2) death from other causes
    
. . .
    
- Typical approach: 
    - Fit cause-specific model for event of interest
    - Treats other events as censored
    - → loses information
    
. . .

**Main goal**: Fit cause-specific model for event 1 _using shared information_ from event 2

## Feature-Weighted Elastic Net


Elastic net objective function with negative log-likelihood contribution for observation $i$:

<!-- $$ -->
<!-- \hat{\beta} = \underset{\beta}{\operatorname{arg min}} \quad \mathrm{NLL}(\beta) + {\color{BIPSBlue}\lambda} \sum_{j=1}^p \left( {\color{BIPSOrange}\alpha} |\beta_j| + \frac{1 - {\color{BIPSOrange}\alpha}}{2} \beta_j^2 \right) -->
<!-- $$ -->

$$
\hat{\beta} = \underset{\beta}{\operatorname{arg min}} \quad \sum_{i=1}^n \ell(y_i, \mathbf{x}_i^\top, \symbf{\beta}) + {\color{BIPSBlue}\lambda} \sum_{j=1}^p \left( {\color{BIPSBlue}\alpha} |\beta_j| + \frac{1 - {\color{BIPSBlue}\alpha}}{2} \beta_j^2 \right)
$$

## Feature-Weighted Elastic Net


Feature-weighted elastic net [@tay2023featureweightedelastic] extends the objective function:

<!-- $$ -->
<!-- \hat{\beta} = \underset{\beta}{\operatorname{arg min}} \quad \mathrm{NLL}(\beta) + {\color{BIPSBlue}\lambda} \sum_{j=1}^p \left( {\color{BIPSOrange}\alpha} |\beta_j| + \frac{1 - {\color{BIPSOrange}\alpha}}{2} \beta_j^2 \right) -->
<!-- $$ -->

$$
\hat{\beta} = \underset{\beta}{\operatorname{arg min}} \quad  \sum_{i=1}^n \ell(y_i, \mathbf{x}_i^\top, \symbf{\beta}) + \lambda \sum_{j=1}^p {\color{BIPSBlue}w_j(\theta)}\left( \alpha |\beta_j| + \frac{1 - \alpha}{2} \beta^2_j \right)
$$

. . .

$$
{\color{BIPSBlue}w_j(\theta)} = \frac{\sum_{l=1}^p \exp(\mathbf{z}_l^\top \theta)}{p \exp(\mathbf{z}_j^\top \theta)}
$$


<!-- [^1]: Tay et al. (2023) -->

<!-- ::: footnote -->
<!-- [@zou2005regularizationvariable; @tay2023featureweightedelastic] -->
<!-- ::: -->


## Feature-Weighted Elastic Net

<!-- - Motivation: Using external/prior information -->

<!-- - Adjust penalization weights on individual or groups of features -->

$$
{\color{BIPSBlue}w_j(\theta)} = \frac{\sum_{l=1}^p \exp(\mathbf{z}_l^\top \theta)}{p \exp(\mathbf{z}_j^\top \theta)}
$$

Assign prior values / groups via matrix $\mathbf{Z} \in \mathbb{R}^{p \times K}$


\vspace{1em}

. . .

::: columns
::: {.column width="50%"}
Examples for grouping: $\mathbf{Z} \in \mathbb{R}^{5 \times 2}$

$$
\mathbf{Z} = 
\begin{pmatrix} 
1 & 0 \\
1 & 0 \\
0 & 1 \\
0 & 1 \\
0 & 1 \\
\end{pmatrix}
$$
:::

. . .

::: {.column width="50%"}
...and individual weighting: $\mathbf{Z} \in \mathbb{R}^{5 \times 1}$
$$
\mathbf{Z} = 
\begin{pmatrix} 
1 \\
1.5 \\
1.2 \\
0.3 \\
0.7 \\
\end{pmatrix}
$$
:::
:::

## Feature-Weighted Elastic Net


<!-- $$ -->
<!-- \hat{\beta} = \underset{\beta}{\operatorname{arg min}} \quad  \sum_{i=1}^n \ell(y_i, \mathbf{x}_i^\top, \symbf{\beta}) + \lambda \sum_{j=1}^p {\color{BIPSBlue}w_j(\theta)}\left( \alpha |\beta_j| + \frac{1 - \alpha}{2} \beta^2_j \right) -->
<!-- $$ -->

$$
w_j(\theta) = \frac{\sum_{l=1}^p \exp(\mathbf{z}_l^\top \theta)}{p \exp(\mathbf{z}_j^\top \theta)}
$$

\vspace{0.75em}

. . .

::: columns
::: {.column width="50%"}
- $\theta \in \mathbb{R}^{K \times 1}$ selected internally

- $w_j(\theta)$ chosen heuristically

:::

. . .

::: {.column width="50%"}
-   $\mathbf{z}_j^\top \theta$ acts as score
-   Larger value $\Rightarrow$ lower $w_j$ feature is _more important_
-   $\theta = 0 \Rightarrow w_j = 1$

:::

:::

<!--
## Penalization Weights $w_j$

- $\theta \in \mathbb{R}^{K \times 1}$ is selected internally

-   Function $w_j(\theta)$ was chosen heuristically by the authors for desirable properties

-   $\mathbf{z}_j^\top \theta$ acts as a *score*

    -   $= 0$, reduces to original elastic net

    -   Higher score $\rightarrow$ lower $w_j$, feature is "more important"

-->

## Individual Feature Weighting

-   $\mathbf{Z} \in \mathbb{R}^{p \times 1}$: Individual weights
-   Simulation from Tay et al.: $\mathbf{Z}$ set to noisy version of true $|\symbf{\beta}|$

. . .

::: {.nonincremental}
-   Large $|\beta_j| \Rightarrow$ weak penalization for $\hat{\beta}_j$
-   $|\beta_j| \approx 0 \Rightarrow$ stronger penalization for $\hat{\beta}_j$
:::

## Application for Multi-Task Learning

Authors suggest multi-task learning algorithm: $\mathbf{X}$ and targets $\mathbf{y}_1, \mathbf{y}_2$

. . .

1.  Set $\symbf{\beta_1}^{(0)}, \symbf{\beta_2}^{(0)}$ to elastic net solution for $(\mathbf{X}, \mathbf{y}_1), (\mathbf{X}, \mathbf{y}_2)$ respectively

. . .

2.  For $k = 0, 1, \ldots$ until stopped:

  a)  $\mathbf{Z}_2 = \left|\symbf{\beta}_1^{(k)}\right|$. Fit `fwelnet` with $(\mathbf{X}, \mathbf{y}_2, \mathbf{Z}_2)$
      -   Set $\left|\symbf{\beta}_2^{(k+1)}\right|$ to solution with optimal $\lambda$

. . .

  b)  $\mathbf{Z}_1 = \left|\symbf{\beta}_2^{(k+1)}\right|$. Fit `fwelnet` with $(\mathbf{X}, \mathbf{y}_1, \mathbf{Z}_1)$
      -   Set $\left|\symbf{\beta}_1^{(k+1)}\right|$ to solution with optimal $\lambda$

## Transfer to Competing Risks


-   Adapt previous algorithm to Cox regression
-   $(\mathbf{X}, \mathbf{y}_e)$ becomes $(\mathbf{X}, \mathbf{t}_e, \delta_e)$

. . .

-   Assumption: Shared information between both events:
    -   If $X_j$ is important for event 1, may also be relevant for event 2
    -   $\Rightarrow$ lower its penalty in cause-specific models
-   Multi-task $\simeq$ "Multi-cause"


\vspace{1em}

. . .

Dubbed "Cooperative Penalized (Cox) Regression" (CooPeR)


## High-Dimensional Variable Selection

- Simulation setup adapted from from @binder2009boostinghighdimensional
- Context: Gene expression data with competing risk target
- n = 400, p = 5000, organized in 4 main blocks
    - 250 variables per block
    - 4 informative variables per block, 16 total
- Use $\mathbf{1}\{\hat{\beta}_j \neq 0\}$ as classification decision

## Simulation of True Effects

- Block 1 (**Mutual**):
    - Same effect on both causes
- Block 2 (**Reversed**):
    - Positive effect on cause 1 and negative on cause 2
- Block 3 (**Disjoint**):
    - 3.1: effect on cause 1 only
    - 3.2: effect on cause 2 only

- Block 4 (**Cor. Noise**): 500 variables, $\rho \approx 0.32$<br>
- Remaining variables: Uncorrelated noise


::: notes
Effects of 0.5 on cause-specific hazard function
 $\rho \approx 0.5$ $\rho \approx 0.35$, $\rho \approx 0.05$
:::


## Positive Predictive Value $\tfrac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FP}}$

<!-- : $\mathrm{PPV} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FP}}$ -->

```{r}
#| fig-align: "center"
#| out-width: "100%"
#| out-height: "70%"
#| fig-asp: 1
knitr::include_graphics("img/cen-selected-ppv-equallambda.pdf")
```

<!--

## Positive Predictive Value: Different Prevalences


```{r}
#| fig-align: "center"
#| out-width: "100%"
#| out-height: "70%"
#| fig-asp: 1
knitr::include_graphics("img/selected-ppv-difflambda.png")
```

::: footer
$$\mathrm{PPV} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FP}}$$
:::

-->

## False Positive Rate $\tfrac{\mathrm{FP}}{\mathrm{FP} + \mathrm{TN}}$

```{r}
#| out-width: "100%"
#| out-height: "70%"
#| fig-align: "center"
#| fig-asp: 1
knitr::include_graphics("img/cen-selected-fpr-equallambda.pdf")
```

::: footer
$$\mathrm{FPR} = \frac{\mathrm{FP}}{\mathrm{FP} + \mathrm{TN}} = 1 - \mathrm{TNR}$$
:::


<!-- 

## False Positive Rate

: $\mathrm{FPR} = \frac{\mathrm{FP}}{\mathrm{FP} + \mathrm{TN}} = 1 - \mathrm{TNR}$ 

```{r}
#| out-width: "85%"
#| out-height: "85%"
#| fig-align: "center"
knitr::include_graphics("img/selected-fpr-difflambda.png")
```

::: footer
$$\mathrm{FPR} = \frac{\mathrm{FP}}{\mathrm{FP} + \mathrm{TN}} = 1 - \mathrm{TNR}$$
:::

-->

## How about real data?

::: {.nonincremental}
- Still WIP
- Idea:
    1. Use algorithms for variable selection
    2. Fit standard cause-specific Cox model using only selected variables
    3. Evaluate prediction performance (tBrier, tAUC)
- Tried bladder cancer data (@dyrskjot2005molecularsignature)
:::

## Open questions
- What about $k > 2$ events? No trivial generalization
:::

<!-- ## Misc -->
<!-- @binder2009boostinghighdimensional -->

<!-- @degenhardt2019evaluationvariable -->

<!-- @dyrskjot2005molecularsignature -->

<!-- @ishwaran2014randomsurvivala -->


