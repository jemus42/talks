---
#title: "Feature-Weighted Elastic Net for Competing Risk Outcomes"
title: "High-Dimensional Variable Selection for Competing Risks with Cooperative Penalized Regression"
subtitle: "«CooPeR»"
author: "Lukas Burk"
institute: "BIPS --- LMU/SLDS --- MCML"
date: "2023-05-12"
format:
    revealjs:
      reference-location: block
      incremental: true
      #slide-number: "c/t" # flattened slide number / total slides (default)
      theme: [default, style.scss]
      center: true
      html-math-method: katex
      embed-resources: true
    beamer:
      incremental: true
      navigation: horizontal
bibliography: references.bib
---

## Motivation

::: {.nonincremental}
- Setting: High-dimensional survival data w/ competing risks<br>
    e.g. time to death from bladder cancer vs. death from other cause
- Typical approach: 
    - Fit cause-specific model(s) for event(s) of interest
    - Treats other events as  censored
    - → lose (potential) shared information
:::

. . .

**Main goal**: Fit cause-specific model for event 1 _using shared information_ from event 2

## Building Blocks

1. Penalized Cox regression
    - Elastic net / feature-weighted elastic net (fwelnet)
2. An iterative Algorithm
    - Adapted from multi-task algorithm by fwelnet authors
3. Assumption of shared information between causes
    - Idea: Some features predictive for event 1 will also be predictive for event 2


## Foundation: Elastic Net {.nonincremental}

The elastic net objective function with some negative log-likelihood term:

$$
\underset{\beta}{\operatorname{arg min}} \quad \mathrm{NLL}(\beta) + {\color{blue}\lambda} \sum_{j=1}^p \left( {\color{red}\alpha} |\beta_j| + \frac{1 - {\color{red}\alpha}}{2} \beta^2_j \right)
$$

::: {.columns} 
::: {.column width="50%"}
::: {.nonincremental}
-   $\color{blue}{\lambda} \in \mathbb{R}_+$ controls overall penalty
-   Higher → larger penalty on all $\beta_j$ *equally*
:::
:::

::: {.column width="50%"}
::: {.nonincremental}
-   $\color{red}{\alpha} \in [0, 1]$ is the mixing parameter
-   $1 \Rightarrow$ only $\ell_1$ penalty (LASSO)
-   $0 \Rightarrow$ only $\ell_2$ penalty (ridge)
:::
:::
:::

::: notes
Flexibility: $\lambda_j$ would not work 
:::

<!--

## Elastic Net: Flexibility?

- What if we don't want to penalize all $\beta_j$ equally?
- This does **not** work:

. . .

$$
\underset{\beta}{\operatorname{arg min}} \quad  \mathrm{NLL}(\beta) + \sum_{j=1}^p {\color{blue}\lambda_j}\left( {\color{red}\alpha} |\beta_j| + \frac{1 - {\color{red}\alpha}}{2} \beta^2_j \right)
$$

. . .

→ Need different approach

-->

## Feature-Weighted Elastic Net (`fwelnet`)

::: {.nonincremental}
-   Motivation: Using external information
-   Adjust penalization weights on individual or groups of features
-   Assign weights / groups via matrix $\mathbf{Z} \in \mathbb{R}^{p \times K}$
:::

<!--
. . .

### Two Applications

1. Assign features to $K$ groups w/ separate penalization weights
2. Adjust penalization weights within group
-->

::: footer
[@tay2023featureweightedelastic]
:::

## Feature Weighting: Groups

Example for $p = 5$ features $X_{1,2,3,4,5}$ and $K = 2$ groups

. . .

::: columns
::: {.column width="40%"}
$$
\mathbf{Z} = 
\begin{pmatrix} 
1 & 0 \\
1 & 0 \\
0 & 1 \\
0 & 1 \\
0 & 1 \\
\end{pmatrix}
$$
:::

::: {.column width="60%"}
::: {.nonincremental}
-   $X_1, X_2$ ➔ group 1
-   $X_3, X_4, X_5$ ➔ group 2
:::
:::
:::

::: notes
Interesting when e.g. $p >> 1000$ w/ 50 clinical + 5000 gene expression features
group-specific penalization weights (c.f. "group lasso" [@jacob2009grouplasso])
:::

## Feature Weighting: Individually

Example for $p = 4$ features $X_{1,2,3,4}$:

. . .

::: columns
::: {.column width="40%"}
$$
\mathbf{Z} = 
\begin{pmatrix} 
0.5 \\
1 \\
2 \\
0 \\
\end{pmatrix}
$$
:::

::: {.column width="60%"}
::: {.nonincremental}
-   $X_1$: Less important, strong penalization
-   $X_2, X_3$: More important → weaker penalization
-   $X_4$: "Irrelevant" → stronger penalization
:::
:::
:::

::: {.notes}

Negative values possible for even stronger penalization but for our purposes not used (see later)
:::

## Feature-Weighting: New Objective Function

$$
\underset{\beta}{\operatorname{arg min}} \quad  \mathrm{NLL}(\beta) + \lambda \sum_{j=1}^p {\color{blue}w_j(\theta)}\left( \alpha |\beta_j| + \frac{1 - \alpha}{2} \beta^2_j \right)
$$


$$
{\color{blue}w_j(\theta)} = \frac{\sum_{l=1}^p \exp(\mathbf{z}_l^T \theta)}{p \exp(\mathbf{z}_j^T \theta)}
$$

. . .

**Penalization weight** of $\beta_j$ based on corresponding value in $\mathbf{Z}$ and parameter $\theta \in \mathbb{R}^{K \times 1}$

::: notes
- $\mathbf{z}_j$: Row $j$ of $\mathbf{Z}$
- $\theta$ is a scalar if $\mathbf{Z} \in \mathbb{R}^{p \times 1}$
:::

## Penalization Weights

-   $w_j(\theta)$ is chosen heuristically by the authors for desirable properties
-   $\mathbf{z}_j^T \theta$ acts as a *score*
    -   $= 0$, reduces to original elastic net
    -   Higher score $\rightarrow$ lower $w_j$, feature is "more important"


::: {.notes}
- w: reduces to elastic net for 0 score, neat
- weights don't become negligible or explode
:::

## Feature-Weighting: Single Group

-   $\mathbf{Z} \in \mathbb{R}^{p \times 1}$: No groups, just weights
-   Simulation from @tay2023featureweightedelastic: $\mathbf{Z}$ set to noisy version of true $|\beta|$

. . .

::: {.nonincremental}
-   Larger $|\beta_j| \Rightarrow$ weaker penalization for $\hat{\beta}_j$
-   $|\beta_j| \approx 0 \Rightarrow$ stronger penalization for $\hat{\beta}_j$
:::

## Application for Multi-Task Learning {.smaller}

Authors suggest multi-task learning algorithm: $\mathbf{X}$ and targets $\mathbf{y}_1, \mathbf{y}_2$

. . .

1.  Set $\beta^{(0)}, \beta^{(0)}$ to `glmnet` solution for $(\mathbf{X}, \mathbf{y}_1), (\mathbf{X}, \mathbf{y}_2)$ respectively

2.  For $k = 0, 1, \ldots$ until stopped:

    a)  $\mathbf{Z}_2 = \left|\beta_1^{(k)}\right|$. Fit `fwelnet` with $(\mathbf{X}, \mathbf{y}_2, \mathbf{Z}_2)$
        -   Set $\left|\beta_2^{(k+1)}\right|$ to solution with optimal `lambda`
    b)  $\mathbf{Z}_1 = \left|\beta_2^{(k+1)}\right|$. Fit `fwelnet` with $(\mathbf{X}, \mathbf{y}_1, \mathbf{Z}_1)$
        -   Set $\left|\beta_1^{(k+1)}\right|$ to solution with optimal `lambda`

::: notes
Side note: $\beta_2^{(0)}$ is never used. Weird.
:::

## Transfer to Competing Risks

::: {.nonincremental}
-   Basic idea: Adapt previous algorithm to Cox regression
-   Setting with two causes/event types: $(\mathbf{t}_1, \delta_1), (\mathbf{t}_2, \delta_2)$
-   Assumption: Shared information for both causes:
    -   If $X_j$ is important for cause 1, may also be relevant for cause 2
    -   $\Rightarrow$ lower its penalty in cause-specific models
-   Multi-task $\simeq$ "Multi-cause"
:::

. . .

::: {.r-fit-text}
Dubbed "Cooperative Penalized (Cox) Regression" (CooPeR)
:::


<!-- ## What We Have -->

<!-- -   1: Adapt `fwelnet` for Coxnet/`Surv` outcome via `glmnet` -->
<!-- -   2: Implemented multi-task algorithm for Cox/CR setting -->
<!-- -   3: Proof of concept:  -->
<!-- -   4: Apply to a high-dimensional CR setting from @binder2009boostinghighdimensional -->
<!-- -   5: Try it on real data -->

<!---

## Proof of Concept

- Apply algorithm to some "easy" simulated data settings:
    -   Large effects ($\beta_j = 1$ or $0.25$ for "small effect")
    -   $N = 1000$
    -   11-13 noise variables

- Settings:
    - A: Equal shared effect 
    - B: No overlap 
    - C: Unequal shared effect, less prevalent cause 2
    - D: Equal shared effects, less prevalent cause 2

##

::: columns

::: {.column width="50%"}
A: Equal shared effect <br>
B: No overlap 
:::

::: {.column width="50%"}
C: Unequal shared effect, less prevalent cause 2 <br>
D: Equal shared effects, less prevalent cause 2
:::

:::


```{r}
#| fig-align: "center"
#| out-width: "75%"
knitr::include_graphics("img/poc-boxplot-errors.png")
```


::: {.notes}
A: Equal shared effect
B: No overlap
C: Unequal shared effect, less prevalent Cause 2
D: Equal w/ multiple
:::

-->

## High-Dimensional Variable Selection

- Simulation setup borrowed from @binder2009boostinghighdimensional
- Context: Gene expression data with competing risk target
- n = 400, p = 5000, organized in 4 main blocks
    - 250 variables per block
    - 4 informative variables per block, 16 total
- Fit models, use $\mathbf{1}\{\hat{\beta}_j \neq 0\}$ as classification decision

## Simulation of True Effects

- Block 1 (**Mutual**):<br>
    same effect on both causes
- Block 2 (**Reversed**): <br>
    positive effect on cause 1 and negative on cause 2
- Block 3 (**Disjoint**):<br>
    3.1: effect on cause 1 only<br>
    3.2: effect on cause 2 only

- Block 4 (**Cor. Noise**): 500 variables, $\rho \approx 0.32$<br>
- Remaining variables: Uncorrelated noise


::: notes
Effects of 0.5 on cause-specific hazard function
 $\rho \approx 0.5$ $\rho \approx 0.35$, $\rho \approx 0.05$
:::

## Positive Predictive Value

<!-- : $\mathrm{PPV} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FP}}$ -->

```{r}
#| out-width: "85%"
#| out-height: "85%"
#| fig-align: "center"
knitr::include_graphics("img/selected-ppv-equallambda.png")
```

::: footer
$$\mathrm{PPV} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FP}}$$
:::

<!--

## Positive Predictive Value: Different Prevalences


```{r}
#| out-width: "85%"
#| out-height: "85%"
#| fig-align: "center"
knitr::include_graphics("img/selected-ppv-difflambda.png")
```

::: footer
$$\mathrm{PPV} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FP}}$$
:::

-->

## False Positive Rate

```{r}
#| out-width: "85%"
#| out-height: "85%"
#| fig-align: "center"
knitr::include_graphics("img/selected-fpr-equallambda.png")
```

::: footer
$$\mathrm{FPR} = \frac{\mathrm{FP}}{\mathrm{FP} + \mathrm{TN}} = 1 - \mathrm{TNR}$$
:::


<!-- 

## False Positive Rate

: $\mathrm{FPR} = \frac{\mathrm{FP}}{\mathrm{FP} + \mathrm{TN}} = 1 - \mathrm{TNR}$ 

```{r}
#| out-width: "85%"
#| out-height: "85%"
#| fig-align: "center"
knitr::include_graphics("img/selected-fpr-difflambda.png")
```

::: footer
$$\mathrm{FPR} = \frac{\mathrm{FP}}{\mathrm{FP} + \mathrm{TN}} = 1 - \mathrm{TNR}$$
:::

-->

## How about real data?

::: {.nonincremental}
- Still WIP
- Idea:
    1. Use algorithms for variable selection
    2. Fit standard cause-specific Cox model using only selected variables
    3. Evaluate prediction performance (tBrier, tAUC)
- Tried bladder cancer data (@dyrskjot2005molecularsignature), did not go well
:::

## Open questions

::: {.nonincremental}
- What does the actual optimization problem look like?<br>
    (Does the algorithm converge? To what?)
- What about $k > 2$ events? No trivial generalization
:::

## References
