---
title: "{mlr3torch}: First Look"
subtitle: "Let's see how this goes" # e.g. the occasion
author: "Lukas Burk" # For the title slide
date: "2021-09-29" # Automatically evaluates to today
output:
  xaringan::moon_reader:
    # css: [bips.css, default]
    css: [xaringan-themer.css, custom-sizing.css]
    lib_dir: js
    nature:
      beforeInit: [macros.js, progress.js]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: "%current%" # Only show current slide number
      ratio: "16:9" # Use "4:3" for old projectors etc.
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  dev = "ragg_png",
  dev.args = list(background = "transparent"),
  fig.align = "center",
  fig.retina = 2,
  fig.height = 5,
  fig.width = 7,
  cache = FALSE
)

# remotes::install_github("gadenbuie/xaringanthemer")
library(xaringanthemer)
style_mono_accent(
  base_color = "#006633",
  background_color = "#FAF9F9",
  header_font_google = google_font("Lato"),
  text_font_google   = google_font("Source Sans Pro", "400", "400i"),
  code_font_google   = google_font("Fira Mono"),
  header_color = lighten_color("#006633", strength = .2),
  link_color = lighten_color("#006633", strength = .2),
  base_font_size = "25px",
  black_color = "#0A0A0A",
  white_color = "#FAFAFA",
  code_font_size = "0.8rem", # Default is 0.9rem
  # Experimental LMU logo stuff, it's hard
  title_slide_background_image = "lmu-black.png",
  title_slide_background_position = "bottom -200px right -200px",
  title_slide_background_size = "40%"
  #background_image = "lmu-gray.png",
  # background_position = "bottom -250px right -250px",
  # background_size = "40%"
)

# xaringanExtra features, see https://pkg.garrickadenbuie.com/xaringanExtra/
xaringanExtra::use_xaringan_extra(
  c("tile_view", "panelset")
)

theme_local <- function(...) {
  theme_xaringan() +
  theme(
    axis.line = element_line(color = "#0A0A0A"),
    panel.background = element_rect(fill = "transparent"), # transparent panel bg
    plot.background = element_rect(fill = "transparent", color = NA), # transparent plot bg
    panel.grid.major = element_blank(), # remove major gridlines
    panel.grid.minor = element_blank(), # remove minor gridlines
    legend.background = element_rect(fill = "transparent"), # transparent legend bg
    legend.box.background = element_rect(fill = "transparent"), # transparent legend panel
    ...
  )
}

library(mlr3)
library(mlr3tuning)
library(ggplot2)
library(dplyr)
library(kableExtra)
```

## First Impressions

- Basic idea: Do what {mlr3keras} does, but s/keras/torch/

--

### Pros

- {torch} is purely based on R and C++
- Heavy lifting is done by `libtorch` in C++ (currently v1.9)
- $\Rightarrow$ No Python dependency, no reticulate mess with conda/venvs

### Cons

- Still very early for {torch} (currently 0.5.0)
- Concerns about performance (will get better, but probably not equal PyTorch)

---

## {mlr3torch}

- Current minimal working example at `jemus42/mlr3torch` (private as of now)

--

- Inside the tin:
    - `LearnerRegrTorchTabnet` + `LearnerClassifTorchTabnet`
    - Wrapping *TabNet* learner from {tabnet} package (`mlverse/tabnet`)
    - $\rightarrow$ simple fitting fun (`tabnet_fit`), hyperparams as simple arguments
    
- Motivation:
    1. Easier to wrap than custom {torch} model building architecture
    2. Allows comparison to similar wrapper in {mlr3keras} for TensorFlow version
    
---

## TabNet in `{mlr3torch}` & `{mlr3keras}`

No extensive comparison so far, purely a proof-of-concept.

- Compared (aggregate) time to
    1. Instantiate learner
    2. Train on `german_credit` classif. task for 10 epochs
    3. Calculate training accuarcy (to ensure reasonably similar results)
- Attempted to set relevant hyperparams to same values (implementation details may vary)

```{r keras-tabnet-bench, fig.width = 12, fig.height = 3}
library(ggplot2)
#library(mlr3viz)
library(microbenchmark)
benchres <- readRDS(here::here("2021/mlr3torch/benchres_german-credit.rds"))

autoplot(benchres) +
  scale_y_continuous(breaks = 1:10, name = "Time [seconds]") +
  theme_minimal(base_size = 16)
```

---

## Tuning Experiment

- Simple grid search over `decision_width` ( $n_d$ ) and `num_steps`
- Search space configuration seems to work fine with
- Quirk: Authors recommend $n_d = n_a$, which `{tabnet}` achieves by setting either value appropriately if `NULL`

```{r tuning-result, eval = FALSE, include = FALSE}
tuninginst <- readRDS(here::here("2021/mlr3torch/tuning-result.rds"))

as.data.table(tuninginst$archive)[, c("x_domain_regr.torch.tabnet.decision_width", "x_domain_regr.torch.tabnet.num_steps", "regr.rmse")] %>%
  as_tibble() %>%
  setNames(c("n_d", "num_steps", "rmse")) %>%
  arrange(rmse)
```

### Caveats

Current limitations in {torch}:

1. CPU: No obvious way to limit CPU threads (?)
2. GPU: No multi-GPU training (yet, this is WIP apparently)
3. GPU: Assigning a specific GPU for training may be possible via env vars?

$\Rightarrow$ No (little?) control over hardware utilization may complicate benchmarking?

---

## Elephant in the Room: {torch} vs. PyTorch

Small benchmark from said [mlverse/torch#268](https://github.com/mlverse/torch/issues/268) compares training loop runtime of small MLP for 

1. {torch}
2. PyTorch imported via {reticulate}
3. Native PyTorch, called from R

```{r pytorch-bench, fig.width = 8, fig.height = 3}
torchbench <- readRDS(here::here("2021/mlr3torch/results-tiny.rds"))

ggplot(torchbench, aes(x = implementation)) +
  geom_errorbar(aes(ymin = lq, max = uq), width = .5) +
  geom_point(aes(y = mean), color = "darkred") +
  scale_y_continuous(breaks = seq(0, 400, 50), minor_breaks = seq(0, 400, 25)) +
  coord_flip() +
  labs(
    y = "Mean (LQ + UQ) [ns]"
  ) +
  theme_minimal(base_size = 16)

ratio <- torchbench[torchbench$implementation == "rtorch", ]$median /
  torchbench[torchbench$implementation == "native_pytorch", ]$median

```

$\Rightarrow$ Native PyTorch outperforms {torch} by a factor of ~`r round(ratio)`

---

### LeNet (simulated data)

```{r bench-lenet, fig.width=9, fig.height=5, out.width = "80%"}
results_lenet <- readRDS(here::here("2021/mlr3torch/results_lenet.rds"))

ratio_lenet <- results_lenet %>%
  select(implementation, epochs, median) %>%
  tidyr::pivot_wider(names_from = implementation, values_from = median) %>%
  arrange(epochs) %>%
  mutate(ratio = rtorch / native_pytorch)

ggplot(results_lenet, aes(x = factor(epochs), y = median, color = implementation)) +
  geom_errorbar(aes(ymin = lq, ymax = uq), width = .5) +
  geom_path(aes(group = implementation)) +
  scale_color_manual(
    values = c(rtorch = "#377EB8", native_pytorch = "#4DAF4A"),
    labels = c(rtorch = "{torch}", native_pytorch = "PyTorch")
  ) +
  scale_y_log10(
    sec.axis = sec_axis(~./1000, name = "Seconds")
  ) +
  labs(
    title = "LeNet: Training Loop Execution Times",
    subtitle = paste0("{torch} to PyTorch ratio: ≈ ", round(mean(ratio_lenet$ratio), 1)),
    x = "# of Epochs",
    y = "Median [log10(ms)] + LQ/UQ",
    color = "Implementation",
    caption = "Runs within microbenchmark at 5 evals"
  ) + 
  theme_minimal() +
  theme(legend.position = "top")
```

---

### ResNet18 + CIFAR10 image classif.

```{r bench-resnetcifar, fig.width=9, fig.height=5, out.width = "80%"}
results_resnetcifar <- readRDS(here::here("2021/mlr3torch/results_resnet18-cifar10.rds"))

ratio_cifar <- results_resnetcifar %>%
  group_by(implementation, batch_size, epochs) %>%
  summarize(mean_time = mean(elapsed), .groups = 'drop') %>%
  tidyr::pivot_wider(names_from = implementation, values_from = mean_time) %>%
  mutate(ratio = rtorch / native_pytorch)


results_resnetcifar %>%
  filter(epochs == 10) %>%
  ggplot(aes(x = factor(batch_size), y = elapsed, color = implementation)) +
  geom_point(size = 4) +
  scale_color_manual(
    values = c(rtorch = "#377EB8", native_pytorch = "#4DAF4A"),
    labels = c(rtorch = "{torch}", native_pytorch = "PyTorch")
  ) +
  scale_y_continuous(
    breaks = seq(0, 2000, 120),
    sec.axis = sec_axis(~./60, name = "Minutes", breaks = seq(0, 20, 2))
  ) +
  labs(
    title = "ResNet18 - CIFAR10: Training Loop Execution Times (10 epochs)",
    subtitle = paste0("{torch} to PyTorch ratio: ≈ ", round(mean(ratio_cifar$ratio), 1)),
    x = "Batch Size",
    y = "Runtime [s]",
    color = "Implementation",
    caption = "Timed manually with tictoc"
  ) + 
  theme_minimal() +
  theme(legend.position = "top")
```

---
### LeNet

```{r results-resnetcifar-tab0}
ratiotab <- bind_rows(
  ratio_lenet %>% 
    mutate(
      model = "LeNet", .before = 1,
      native_pytorch = round(native_pytorch / 1000, 2),
      rtorch = round(rtorch / 1000, 2)
    ),
  ratio_cifar %>% 
    mutate(
      model = "ResNet18 + CIFAR10", .before = 1,
      native_pytorch = round(native_pytorch, 2),
      rtorch = round(rtorch, 2)
    )
) %>%
  mutate(
    ratio = round(ratio, 1)
  )
```


```{r results-resnetcifar-tab1}
ratiotab %>%
  select(-batch_size) %>%
  filter(model == "LeNet") %>%
  kable(caption = "LeNet") %>%
  kable_styling()
```

---
### ResNet18 + CIFAR10

```{r results-resnetcifar-tab2}
ratiotab %>%
  select(model, epochs, batch_size, everything()) %>%
  filter(model == "ResNet18 + CIFAR10") %>%
  arrange(epochs, batch_size) %>%
  kable(caption = "ResNet18 + CIFAR10") %>%
  kable_styling()
```

---

## Outlook

- Primary pain points: `dataloader()` & optimizer steps

- See also [#694](https://github.com/mlverse/torch/issues/694)

> We still need to make performance improvements in the R side. Specially related to **dataloading** and in the **optimizers** code.

