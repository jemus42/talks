---
title: "Neural Network Packages in R"
subtitle: "Wiesbaden UseR Coffee Break"
author: "Lukas Burk"
slides: "https://slides.lukasburk.de/2021/nnpkgs"
contactauthor: "Lukas Burk"
contactemail: "burk@leibniz-bips.de"
date: "2021-06-30"
output:
  xaringan::moon_reader:
    css: [bips.css, default]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: "%current%" # Only show current slide number
      ratio: "16:9" # Use "4:3" for old projectors etc.
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,          # Set FALSE to hide R code
  messages = FALSE,     # Drops e.g. pkg startup messages on library() calls
  warning = FALSE,      # Don't show warnings from R code
  dev = "ragg_png",     # Higher quality png graphics device
  fig.align = "center", # Centered plots (recommended)
  fig.retina = 2,       # Higher image quality for high resolution screens
  cache = TRUE
)

# xaringanExtra features, see https://pkg.garrickadenbuie.com/xaringanExtra/
xaringanExtra::use_xaringan_extra(
  c("tile_view", "panelset", "clipboard")
)
xaringanExtra::use_progress_bar(color = "#1763AA", location = "bottom")
xaringanExtra::use_extra_styles(
  hover_code_line = TRUE
)
```

class: middle

## Hi, I'm Lukas üëã

- Fresh PhD student @ BIPS Bremen & LMU M√ºnchen (Since April '21) 

--

- Context: Biostatistics / {M,D}L / Data Science

--

- First steps in R: 2014

--

- First neural network: 2020

---
class: inverse, middle

# Neural Network Packages

---

## The Three Biggies

- `{nnet}`: The grandpa of neural networks in R.

    - Mostly historically relevant, less useful for modern tasks
    - Limited to single-layer neural networks ("90s neural nets") 
    
--

- `{keras}` (as interface to `{tensorflow}`): Established go-to framework.

    - You'll find *tons* of examples in `{keras}` üòÅ
    - You'll likely run into issues that lead to the underlying Python implementation üôÉ

--

- `{torch}` with `{luz}`: New kid on the block, the one to keep an eye on (imo)

    - R bindings to `libtorch` (C++), same backend as `PyTorch`
    - R package is still very young! Currently 0.4.0 üë∂
    - `{luz}` for `keras`-like convenience

---
class: inverse, middle
# Neural Networks in .bipsorange[nnet]

---

# Example Task: Penguins

When in doubt, [palmer penguins](https://allisonhorst.github.io/palmerpenguins/) üêß

```{r, message=FALSE, warning=FALSE}
library(tidyverse); library(palmerpenguins)
penguins <- slice_sample(penguins, prop = 1)
head(penguins)
```

---
## Penguin Preparation

- 80/20 train-test-split

- Scaling for more manageable gradients

```{r peng-traintestsplit}
penguins_train <- penguins[1:276, ]  # ~80% #<<
penguins_test <- penguins[277:344, ] # ~20%

penguins_train <- penguins_train %>% mutate(across(where(is.numeric), scale)) #<<
penguins_test <- penguins_test %>% mutate(across(where(is.numeric), scale))
```

---
## Model Fit with `nnet`

- Familiar formula interface üëå 
- Automatic handling of `factor` variables üëç
- Similar to `lm()` and friends

```{r nnet-class-fit, message=FALSE}
library(nnet)
mod_nnet <- nnet(
  island ~ bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g, #<<
  data = penguins_train,
  size = 10,    # Units in the hidden layer #<<
  trace = FALSE # Shut up about it
)
```



---
## Predictions with `nnet`

- No surprises here

```{r nnet-class-pred}
island_pred <- predict( #<<
  mod_nnet, 
  type = "class", #<<
  newdata = select(penguins_test, bill_length_mm:body_mass_g)
)

# Individual class predictions
island_pred[1:5]
```


---
class: inverse, middle
# Neural Networks in .bipsorange[tensorflow] and .bipsorange[keras]

---
## Example Task: MNIST

- Handwritten image recognition

- The "Hello World" of deep learning

- 3D arrays of 60k images, 28x28 greyscale matrices (0-255)

--

```{r keras-load-mnist}
library(keras)

mnist <- dataset_mnist()
str(mnist)
```

---
## Example Digit

.pull-left[
```{r keras-mnist-example-digit}
digit <- mnist$train$x[1,,]
dim(digit)
```

```{r, eval=FALSE}
plot(as.raster(digit, max = 255))
```
]

.pull-right[
```{r mnist-digit, echo=FALSE}
plot(as.raster(digit, max = 255))
```

]

---
## Preprocessing with `keras` (1)

- Reshaping via `keras::array_reshape()`

- Images are a matrix with 28x28 = **784 colums**, one row per image

--

```{r keras-preprocess-1}
c(c(train_images, train_labels), c(test_images, test_labels)) %<-% mnist

train_images <- array_reshape(train_images, c(60000, 28 * 28)) #<<
test_images <- array_reshape(test_images, c(10000, 28 * 28))
```

---
## Preprocessing with `keras` (2)

- Scale images to 0-1

- Encode labels to categorical shapes (`keras::to_categorical`)


```{r keras-preprpcess-2, dependson=c(-1)}
train_images <- train_images / 255 #<<
test_images <- test_images / 255

train_labels <- to_categorical(train_labels) #<<
test_labels <- to_categorical(test_labels)
```


```{r keras-preprpcess-3, include=FALSE, dependson=c(-1)}
obs <- nrow(train_images)
set.seed(123)
randomize <- sample(seq_len(obs), size = obs, replace = FALSE)
train_images <- train_images[randomize, ]
train_labels <- train_labels[randomize, ]
```

---
## Build a Model in `keras`

- `keras_model_sequential` is the easiest way to get started

- `layer_dense` also defines activation function


```{r keras-build-compile-1, dependson=c(-1)}
model <- keras_model_sequential(name = "MNIST-MLP") %>%
  layer_dense(units = 64, activation = "relu", input_shape = 784) %>% #<<
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 10, activation = "softmax", name = "Output") #<<
```

.footnote[Note: No spaces in the `name` or you'll get unhelpful errors üôÉ]

---
## `compile()` a `keras`  Model

- Defines loss function, optimizer (with parameters), and other metrics

- Use either `"adam"` *or* `optimizer_adam(...)` and set parameters


```{r keras-build-compile-2, dependson='keras-build-compile-1'}
model %>% compile(
  loss = "categorical_crossentropy", #<<
  optimizer = "adam",                #<<
  metrics = "accuracy" # Optional
)
```

---
## Take a Look at a `keras` Model

```{r keras-build-summary, dependson='keras-build-compile-2'}
summary(model)
```

---
## Fit a `keras` Model

- Supply training images and labels (distinct, no formula interface)

- Set number of `epochs`, `batch_size`, ...

- Save training `history` for inspection


```{r keras-fit, dependson='keras-build-compile-2'}
history <- model %>% 
  fit( #<<
    x = train_images, y = train_labels, #<<
    epochs = 5,         # Very short run #<<
    batch_size = 128,   # Big batches for faster training
    validation_split = 0.2
  )
```

---
## Look at the Training History

.pull-left[

- Interactively, you'll get a live-updating plot üòé

- `history` can has a helpful `plot` method for `{ggplot2}`

```{r keras-view-history1, eval=FALSE}
plot(history)
```

]

.pull-right[

```{r keras-view-history2, echo=FALSE, out.width='85%'}
knitr::include_graphics("index_files/figure-html/keras-view-history2-1.png")
```

]

---
## Evaluate a `keras` Model

.pull-left[

- Very simple to get relevant metrics

```{r keras-evaluate, dependson='keras-fit'}
model %>% 
  evaluate(
    test_images, test_labels, 
    verbose = FALSE
  )
```

]

--

.pull-right[

- Further investigation via e.g. a confusion matrix

```{r keras-evaluate-confmat, eval=FALSE}
preds <- model %>% 
  predict_classes(test_images) #<<

actual <- mnist$test$y
caret::confusionMatrix(
  factor(preds), 
  factor(actual)
)
```

]


---
class: inverse, middle
# Neural Networks in .bipsorange[torch] and .bipsorange[luz]

---
## Example Task: Cats and Dogs

```{r torch-pkgs}
library(torch)
library(torchvision)
library(torchdatasets)
library(luz) 
```

- Using a kaggle dataset requires account + phone verification!

```{r torch-catdog-data, eval=FALSE}
ds <- torchdatasets::dogs_vs_cats_dataset( #<<
  root = "data",
  token = "~/.kaggle/kaggle.json", #<<
  transform = . %>% #<<
    torchvision::transform_to_tensor() %>%
    torchvision::transform_resize(size = c(224, 224)) %>% 
    torchvision::transform_normalize(rep(0.5, 3), rep(0.5, 3)),
  target_transform = function(x) as.double(x) - 1
)
```

---
## Preprocessing in `torch`

- Shuffle training data, split train/test
- `dataset_subset` for syntax sugar
- `dataloader` created from data for efficiency

```{r torch-dataset, eval=FALSE}
train_ids <- sample(1:length(ds), size = 0.6 * length(ds))
valid_ids <- sample(setdiff(1:length(ds), train_ids), size = 0.2 * length(ds))
test_ids <- setdiff(1:length(ds), union(train_ids, valid_ids))

train_ds <- dataset_subset(ds, indices = train_ids) #<<
valid_ds <- dataset_subset(ds, indices = valid_ids)
test_ds <- dataset_subset(ds, indices = test_ids)

train_dl <- dataloader(train_ds, batch_size = 64, shuffle = TRUE, num_workers = 4) #<<
valid_dl <- dataloader(valid_ds, batch_size = 64, num_workers = 4)
test_dl <- dataloader(test_ds, batch_size = 64, num_workers = 4)
```

---
## Build `torch` Model with `luz`

.pull-left[

- Network components are called `modules` in `{torch}`

- Plugging in a pre-trained model is easy(ish)

- Sequential layers similar to `keras` are a `luz` bonus üòé

]

--

.pull-right[

.code70[

```{r torch-model, eval=FALSE}
net <- nn_module( #<<
  initialize = function(output_size) { #<<
    self$model <- model_alexnet(pretrained = TRUE)
    for (par in self$parameters) par$requires_grad_(FALSE)
    
    self$model$classifier <- nn_sequential( #<<
      nn_dropout(0.5),
      nn_linear(9216, 512), #<<
      nn_relu(),            #<<
      nn_linear(512, 256),
      nn_relu(),
      nn_linear(256, output_size)
    )
  },
  forward = function(x) { #<<
    self$model(x)[,1]
  }
)
```

]
]

---
## Train `torch` Model with `luz`

- `luz::setup` is similar to `keras::compile`

- Both packages have a nice `fit` method

```{r torch-train, eval=FALSE}
fitted <- net %>%
  setup( #<<
    loss = nn_bce_with_logits_loss(), #<<
    optimizer = optim_adam,           #<<
    metrics = list(luz_metric_binary_accuracy_with_logits())
  ) %>%
  set_hparams(output_size = 1) %>% # param for initialize method
  set_opt_hparams(lr = 0.01) %>% # params for optimizer (adam)
  fit( #<<
    data = train_dl, valid_data = valid_dl #<<
    epochs = 3
  )
```


```{r luz-save, eval=FALSE, include=FALSE}
# Optional saving
luz_save(fitted, "dogs-and-cats.pt")
```

---
## Predicting with `torch`

- `predict` works as usual

- Explicit sigmoid-transformation required to get class-probabilities

```{r luz-load, include=FALSE, eval=FALSE}
fitted <- luz_load(here::here("dogs-and-cats.pt"))
```


```{r torch-predict, eval=FALSE}
preds <- predict(fitted, test_dl)

probs <- torch_sigmoid(preds) #<<
```


---
class: inverse, middle
# Takeaways

---
class: middle

## Long story short

- `nnet` is easy but very limited

--

    - Ideal for "I just want to train **some** neural nets"

--

- `{tensorflow}` and `{keras}` are established, very powerful, lots of examples available

--

    - The Python dependency can cause headaches 

--

- `{torch}` and `{luz}` are very young, but also very promising

--

    - No Python dependency, less (or less painful) headaches üôèüèª 
    - More assembly required than with `{keras}`, can be a good or a bad thing


---
class: middle

## References

- `{keras}` example: https://tensorflow.rstudio.com/guide/keras/examples/mnist_mlp/

    - All things `{keras}` at https://keras.rstudio.com/  
    (Documentation may lag behind Python version üòï)

- `{luz}` example: https://blogs.rstudio.com/ai/posts/2021-06-17-luz/

    - All things `{torch}` at https://mlverse.github.io/  
    (Very much in active development!)

---
class: center, middle, thanks
background-image: none

## Thank you for your attention

`r rmarkdown::metadata$slides`

.pull-left[.right[
.font150[**Contact**]  
.bipsblue[`r rmarkdown::metadata$contactauthor`]  
Leibniz Institute for Prevention Research and Epidemiology - BIPS GmbH  
Achterstra√üe 30  
D-28359 Bremen  
.bipsblue[`r rmarkdown::metadata$contactemail`]
]]
.pull-right[

```{r, echo = FALSE, out.width = "50%"}
knitr::include_graphics("bips-logo.png")
```

]
