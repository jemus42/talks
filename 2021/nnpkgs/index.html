<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Neural Network Packages in R</title>
    <meta charset="utf-8" />
    <meta name="author" content="Lukas Burk" />
    <meta name="date" content="2021-06-30" />
    <script src="libs/header-attrs-2.9/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/tile-view-0.2.6/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view-0.2.6/tile-view.js"></script>
    <link href="libs/panelset-0.2.6/panelset.css" rel="stylesheet" />
    <script src="libs/panelset-0.2.6/panelset.js"></script>
    <script src="libs/clipboard-2.0.6/clipboard.min.js"></script>
    <link href="libs/xaringanExtra-clipboard-0.2.6/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-clipboard-0.2.6/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"Copy Code","success":"Copied!","error":"Press Ctrl+C to Copy"})</script>
    <script src="libs/xaringanExtra-progressBar-0.0.1/progress-bar.js"></script>
    <link href="libs/xaringanExtra-extra-styles-0.2.6/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <link rel="stylesheet" href="bips.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Neural Network Packages in R
## Wiesbaden UseR Coffee Break
### Lukas Burk
### 2021-06-30

---




class: middle

## Hi, I'm Lukas üëã

- Fresh PhD student @ BIPS Bremen &amp; LMU M√ºnchen (Since April '21) 

--

- Context: Biostatistics / {M,D}L / Data Science

--

- First steps in R: 2014

--

- First neural network: 2020

---
class: inverse, middle

# Neural Network Packages

---

## The Three Biggies

- `{nnet}`: The grandpa of neural networks in R.

    - Mostly historically relevant, less useful for modern tasks
    - Limited to single-layer neural networks ("90s neural nets") 
    
--

- `{keras}` (as interface to `{tensorflow}`): Established go-to framework.

    - You'll find *tons* of examples in `{keras}` üòÅ
    - You'll likely run into issues that lead to the underlying Python implementation üôÉ

--

- `{torch}` with `{luz}`: New kid on the block, the one to keep an eye on (imo)

    - R bindings to `libtorch` (C++), same backend as `PyTorch`
    - R package is still very young! Currently 0.4.0 üë∂
    - `{luz}` for `keras`-like convenience

---
class: inverse, middle
# Neural Networks in .bipsorange[nnet]

---

# Example Task: Penguins

When in doubt, [palmer penguins](https://allisonhorst.github.io/palmerpenguins/) üêß


```r
library(tidyverse); library(palmerpenguins)
penguins &lt;- slice_sample(penguins, prop = 1)
head(penguins)
```

```
## # A tibble: 6 x 8
##   species island bill_length_mm bill_depth_mm flipper_length_‚Ä¶ body_mass_g sex  
##   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;            &lt;int&gt;       &lt;int&gt; &lt;fct&gt;
## 1 Gentoo  Biscoe           46.2          14.5              209        4800 fema‚Ä¶
## 2 Gentoo  Biscoe           44.5          14.7              214        4850 fema‚Ä¶
## 3 Gentoo  Biscoe           45.1          14.4              210        4400 fema‚Ä¶
## 4 Adelie  Dream            40.3          18.5              196        4350 male 
## 5 Chinst‚Ä¶ Dream            45.2          16.6              191        3250 fema‚Ä¶
## 6 Adelie  Dream            40.8          18.4              195        3900 male 
## # ‚Ä¶ with 1 more variable: year &lt;int&gt;
```

---
## Penguin Preparation

- 80/20 train-test-split

- Scaling for more manageable gradients


```r
*penguins_train &lt;- penguins[1:276, ]  # ~80%
penguins_test &lt;- penguins[277:344, ] # ~20%

*penguins_train &lt;- penguins_train %&gt;% mutate(across(where(is.numeric), scale))
penguins_test &lt;- penguins_test %&gt;% mutate(across(where(is.numeric), scale))
```

---
## Model Fit with `nnet`

- Familiar formula interface üëå 
- Automatic handling of `factor` variables üëç
- Similar to `lm()` and friends


```r
library(nnet)
mod_nnet &lt;- nnet(
* island ~ bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g,
  data = penguins_train,
* size = 10,    # Units in the hidden layer
  trace = FALSE # Shut up about it
)
```



---
## Predictions with `nnet`

- No surprises here


```r
*island_pred &lt;- predict(
  mod_nnet, 
* type = "class",
  newdata = select(penguins_test, bill_length_mm:body_mass_g)
)

# Individual class predictions
island_pred[1:5]
```

```
## [1] "Biscoe" "Dream"  "Biscoe" "Biscoe" "Dream"
```


---
class: inverse, middle
# Neural Networks in .bipsorange[tensorflow] and .bipsorange[keras]

---
## Example Task: MNIST

- Handwritten image recognition

- The "Hello World" of deep learning

- 3D arrays of 60k images, 28x28 greyscale matrices (0-255)

--


```r
library(keras)

mnist &lt;- dataset_mnist()
str(mnist)
```

```
## List of 2
##  $ train:List of 2
##   ..$ x: int [1:60000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 ...
##   ..$ y: int [1:60000(1d)] 5 0 4 1 9 2 1 3 1 4 ...
##  $ test :List of 2
##   ..$ x: int [1:10000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 ...
##   ..$ y: int [1:10000(1d)] 7 2 1 0 4 1 4 9 5 9 ...
```

---
## Example Digit

.pull-left[

```r
digit &lt;- mnist$train$x[1,,]
dim(digit)
```

```
## [1] 28 28
```


```r
plot(as.raster(digit, max = 255))
```
]

.pull-right[
&lt;img src="index_files/figure-html/mnist-digit-1.png" width="504" style="display: block; margin: auto;" /&gt;

]

---
## Preprocessing with `keras` (1)

- Reshaping via `keras::array_reshape()`

- Images are a matrix with 28x28 = **784 colums**, one row per image

--


```r
c(c(train_images, train_labels), c(test_images, test_labels)) %&lt;-% mnist

*train_images &lt;- array_reshape(train_images, c(60000, 28 * 28))
test_images &lt;- array_reshape(test_images, c(10000, 28 * 28))
```

---
## Preprocessing with `keras` (2)

- Scale images to 0-1

- Encode labels to categorical shapes (`keras::to_categorical`)



```r
*train_images &lt;- train_images / 255
test_images &lt;- test_images / 255

*train_labels &lt;- to_categorical(train_labels)
test_labels &lt;- to_categorical(test_labels)
```




---
## Build a Model in `keras`

- `keras_model_sequential` is the easiest way to get started

- `layer_dense` also defines activation function



```r
model &lt;- keras_model_sequential(name = "MNIST-MLP") %&gt;%
* layer_dense(units = 64, activation = "relu", input_shape = 784) %&gt;%
  layer_dropout(rate = 0.2) %&gt;%
  layer_dense(units = 32, activation = "relu") %&gt;%
  layer_dropout(rate = 0.2) %&gt;%
* layer_dense(units = 10, activation = "softmax", name = "Output")
```

.footnote[Note: No spaces in the `name` or you'll get unhelpful errors üôÉ]

---
## `compile()` a `keras`  Model

- Defines loss function, optimizer (with parameters), and other metrics

- Use either `"adam"` *or* `optimizer_adam(...)` and set parameters



```r
model %&gt;% compile(
* loss = "categorical_crossentropy",
* optimizer = "adam",
  metrics = "accuracy" # Optional
)
```

---
## Take a Look at a `keras` Model


```r
summary(model)
```

```
## Model: "MNIST-MLP"
## _________________________________________________________________________________________________________________________
## Layer (type)                                          Output Shape                                    Param #            
## =========================================================================================================================
## dense_1 (Dense)                                       (None, 64)                                      50240              
## _________________________________________________________________________________________________________________________
## dropout_1 (Dropout)                                   (None, 64)                                      0                  
## _________________________________________________________________________________________________________________________
## dense (Dense)                                         (None, 32)                                      2080               
## _________________________________________________________________________________________________________________________
## dropout (Dropout)                                     (None, 32)                                      0                  
## _________________________________________________________________________________________________________________________
## Output (Dense)                                        (None, 10)                                      330                
## =========================================================================================================================
## Total params: 52,650
## Trainable params: 52,650
## Non-trainable params: 0
## _________________________________________________________________________________________________________________________
```

---
## Fit a `keras` Model

- Supply training images and labels (distinct, no formula interface)

- Set number of `epochs`, `batch_size`, ...

- Save training `history` for inspection



```r
history &lt;- model %&gt;% 
* fit(
*   x = train_images, y = train_labels,
*   epochs = 3,             # Very short run
    batch_size = 128,        # Big batches for faster training
    validation_split = 0.2
  )
```

---
## Look at the Training History

.pull-left[

- Interactively, you'll get a live-updating plot üòé

- `history` can has a helpful `plot` method for `{ggplot2}`


```r
plot(history)
```

]

.pull-right[

&lt;img src="index_files/figure-html/keras-view-history2-1.png" width="85%" style="display: block; margin: auto;" /&gt;

]

---
## Evaluate a `keras` Model

.pull-left[

- Very simple to get relevant metrics


```r
model %&gt;% 
  evaluate(
    test_images, test_labels, 
    verbose = FALSE
  )
```

```
## $loss
## [1] 0.153862
## 
## $accuracy
## [1] 0.9545
```

]

--

.pull-right[

- Further investigation via e.g. a confusion matrix


```r
preds &lt;- model %&gt;% 
* predict_classes(test_images)

actual &lt;- mnist$test$y
caret::confusionMatrix(
  factor(preds), 
  factor(actual)
)
```

]


---
class: inverse, middle
# Neural Networks in .bipsorange[torch] and .bipsorange[luz]

---
## Example Task: Cats and Dogs


```r
library(torch)
library(torchvision)
library(torchdatasets)
library(luz) 
```

- Using a kaggle dataset requires account + phone verification!


```r
*ds &lt;- torchdatasets::dogs_vs_cats_dataset(
  root = "data",
* token = "~/.kaggle/kaggle.json",
* transform = . %&gt;%
    torchvision::transform_to_tensor() %&gt;%
    torchvision::transform_resize(size = c(224, 224)) %&gt;% 
    torchvision::transform_normalize(rep(0.5, 3), rep(0.5, 3)),
  target_transform = function(x) as.double(x) - 1
)
```

---
## Preprocessing in `torch`

- Shuffle training data, split train/test
- `dataset_subset` for syntax sugar
- `dataloader` created from data for efficiency


```r
train_ids &lt;- sample(1:length(ds), size = 0.6 * length(ds))
valid_ids &lt;- sample(setdiff(1:length(ds), train_ids), size = 0.2 * length(ds))
test_ids &lt;- setdiff(1:length(ds), union(train_ids, valid_ids))

*train_ds &lt;- dataset_subset(ds, indices = train_ids)
valid_ds &lt;- dataset_subset(ds, indices = valid_ids)
test_ds &lt;- dataset_subset(ds, indices = test_ids)

*train_dl &lt;- dataloader(train_ds, batch_size = 64, shuffle = TRUE, num_workers = 4)
valid_dl &lt;- dataloader(valid_ds, batch_size = 64, num_workers = 4)
test_dl &lt;- dataloader(test_ds, batch_size = 64, num_workers = 4)
```

---
## Build `torch` Model with `luz`

.pull-left[

- Network components are called `modules` in `{torch}`

- Plugging in a pre-trained model is easy(ish)

- Sequential layers similar to `keras` are a `luz` bonus üòé

]

--

.pull-right[

.code70[


```r
*net &lt;- nn_module(
* initialize = function(output_size) {
    self$model &lt;- model_alexnet(pretrained = TRUE)
    for (par in self$parameters) par$requires_grad_(FALSE)
    
*   self$model$classifier &lt;- nn_sequential(
      nn_dropout(0.5),
*     nn_linear(9216, 512),
*     nn_relu(),
      nn_linear(512, 256),
      nn_relu(),
      nn_linear(256, output_size)
    )
  },
* forward = function(x) {
    self$model(x)[,1]
  }
)
```

]
]

---
## Train `torch` Model with `luz`

- `luz::setup` is similar to `keras::compile`

- Both packages have a nice `fit` method


```r
fitted &lt;- net %&gt;%
* setup(
*   loss = nn_bce_with_logits_loss(),
*   optimizer = optim_adam,
    metrics = list(luz_metric_binary_accuracy_with_logits())
  ) %&gt;%
  set_hparams(output_size = 1) %&gt;% # param for initialize method
  set_opt_hparams(lr = 0.01) %&gt;% # params for optimizer (adam)
* fit(
*   data = train_dl, valid_data = valid_dl
    epochs = 3
  )
```




---
## Predicting with `torch`

- `predict` works as usual

- Explicit sigmoid-transformation required to get class-probabilities





```r
preds &lt;- predict(fitted, test_dl)

*probs &lt;- torch_sigmoid(preds)
```


---
class: inverse, middle
# Takeaways

---
class: middle

## Long story short

- `nnet` is easy but very limited

--

    - Ideal for "I just want to train **some** neural nets"

--

- `{tensorflow}` and `{keras}` are established, very powerful, lots of examples available

--

    - The Python dependency can cause headaches 

--

- `{torch}` and `{luz}` are very young, but also very promising

--

    - No Python dependency, less (or less painful) headaches üôèüèª 
    - More assembly required than with `{keras}`, can be a good or a bad thing


---
class: middle

## References

- `{keras}` example: https://tensorflow.rstudio.com/guide/keras/examples/mnist_mlp/

    - All things `{keras}` at https://keras.rstudio.com/  
    (Documentation may lag behind Python version üòï)

- `{luz}` example: https://blogs.rstudio.com/ai/posts/2021-06-17-luz/

    - All things `{torch}` at https://mlverse.github.io/  
    (Very much in active development!)

---
class: center, middle, thanks
background-image: none

## Thank you for your attention

http://slides.lukasburk.de/2021/nnpkgs

.pull-left[.right[
.font150[**Contact**]  
.bipsblue[Lukas Burk]  
Leibniz Institute for Prevention Research and Epidemiology - BIPS GmbH  
Achterstra√üe 30  
D-28359 Bremen  
.bipsblue[burk@leibniz-bips.de]
]]
.pull-right[

&lt;img src="bips-logo.png" width="50%" style="display: block; margin: auto;" /&gt;

]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"slideNumberFormat": "%current%",
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
