<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Neural Network Packages in R</title>
    <meta charset="utf-8" />
    <meta name="author" content="Lukas Burk" />
    <meta name="date" content="2021-06-30" />
    <script src="libs/header-attrs-2.9/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/tile-view-0.2.6/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view-0.2.6/tile-view.js"></script>
    <link href="libs/panelset-0.2.6/panelset.css" rel="stylesheet" />
    <script src="libs/panelset-0.2.6/panelset.js"></script>
    <script src="libs/clipboard-2.0.6/clipboard.min.js"></script>
    <link href="libs/xaringanExtra-clipboard-0.2.6/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-clipboard-0.2.6/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"Copy Code","success":"Copied!","error":"Press Ctrl+C to Copy"})</script>
    <script src="libs/xaringanExtra-progressBar-0.0.1/progress-bar.js"></script>
    <link href="libs/xaringanExtra-extra-styles-0.2.6/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <link rel="stylesheet" href="bips.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Neural Network Packages in R
## Wiesbaden UseR Coffee Break
### Lukas Burk
### 2021-06-30

---




class: middle

## Hi üëã

- Lukas, fresh PhD student @ BIPS Bremen &amp; LMU M√ºnchen (Since April '21) 

--

- Context: Something something biostatistics / {M,D}L / Data Science

--

- First steps in R: 2014

--

- First neural network: 2020

---
class: inverse, middle

# Neural Network Packages

---

## The Three Biggies

- `{nnet}`: The grandpa of neural networks in R.

    - Mostly historically relevant, less useful for state of the art tasks
    - Limited to single-layer neural networks ("90s neural nets") 
    
--

- `{keras}` (as interface to `{tensorflow}`): Established go-to framework.

    - You'll find *tons* of examples in `{keras}` üòÅ
    - You'll likely run into issues that lead to the underlying Python implementation üôÉ

--

- `{torch}` with `{luz}`: New kid on the block, the one to keep an eye on (imo)

    - R bindings to `libtorch` (C++), same backend as `PyTorch`
    - R package is still very young! Currently 0.4.0 üë∂
    - `{luz}` for `keras`-like convenience

---
class: inverse, middle
# Neural Networks in .bipsorange[nnet]

---

# Example Task: Penguins

When in doubt, [palmer penguins](https://allisonhorst.github.io/palmerpenguins/) üêß


```r
library(tidyverse); library(palmerpenguins)
penguins &lt;- slice_sample(penguins, prop = 1)
head(penguins)
```

```
## # A tibble: 6 x 8
##   species island bill_length_mm bill_depth_mm flipper_length_‚Ä¶ body_mass_g sex  
##   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;            &lt;int&gt;       &lt;int&gt; &lt;fct&gt;
## 1 Adelie  Dream            36            17.9              190        3450 fema‚Ä¶
## 2 Gentoo  Biscoe           46.9          14.6              222        4875 fema‚Ä¶
## 3 Chinst‚Ä¶ Dream            50.9          17.9              196        3675 fema‚Ä¶
## 4 Gentoo  Biscoe           49.4          15.8              216        4925 male 
## 5 Adelie  Dream            36.9          18.6              189        3500 fema‚Ä¶
## 6 Gentoo  Biscoe           50.5          15.9              225        5400 male 
## # ‚Ä¶ with 1 more variable: year &lt;int&gt;
```

---
## Penguin Preparation

- Common 80/20 train-test-split
- Scaling, to be nice to our gradients ü§ó


```r
penguins_train &lt;- penguins[1:276, ]  # ~80%
penguins_test &lt;- penguins[277:344, ] # ~20%

penguins_train &lt;- penguins_train %&gt;% mutate(across(where(is.numeric), scale))
penguins_test &lt;- penguins_test %&gt;% mutate(across(where(is.numeric), scale))
```

---
## Model Fit with `nnet`

- Familiar formula interface üëå 
- No manual re-encoding of `factor` variable needed üëç


```r
library(nnet)
mod_nnet &lt;- nnet(
  island ~ bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g, 
  data = penguins_train,
  size = 10 # Units in the hidden layer
)
```

```
## # weights:  83
## initial  value 394.348405 
## iter  10 value 141.996620
## iter  20 value 117.959218
## iter  30 value 100.424258
## iter  40 value 90.575148
## iter  50 value 82.155739
## iter  60 value 79.133002
## iter  70 value 78.781438
## iter  80 value 78.311187
## iter  90 value 77.399593
## iter 100 value 77.281558
## final  value 77.281558 
## stopped after 100 iterations
```

---
## Prediction with `nnet`

- Same familiar `predict` function


```r
pred_data &lt;- penguins_test %&gt;% select(island, bill_length_mm:body_mass_g)

island_pred &lt;- predict(
  mod_nnet,
  newdata = select(pred_data, bill_length_mm:body_mass_g),
  type = "class"
)

# Individual class predictions
island_pred[1:5]
```

```
## [1] "Dream"     "Dream"     "Dream"     "Dream"     "Torgersen"
```

---
## Evaluating `nnet` Models (1)

- Add our predictions to the original data for a glance


```r
pred_data %&gt;%
  mutate(
    island_pred = island_pred, .before = island,
  ) %&gt;%
  head(8)
```

```
## # A tibble: 8 x 6
##   island_pred island    bill_length_mm[,1] bill_depth_mm[,‚Ä¶ flipper_length_mm[,‚Ä¶
##   &lt;chr&gt;       &lt;fct&gt;                  &lt;dbl&gt;            &lt;dbl&gt;                &lt;dbl&gt;
## 1 Dream       Dream                 -0.147            0.626               -0.532
## 2 Dream       Dream                  1.64             1.40                 0.342
## 3 Dream       Torgersen             -0.668           -0.201               -1.61 
## 4 Dream       Torgersen             -1.19             1.12                -1.07 
## 5 Torgersen   Biscoe                -1.57             0.295               -0.667
## 6 Dream       Dream                  0.860            1.18                 0.679
## 7 Dream       Biscoe                -1.33            -0.146               -1.07 
## 8 Biscoe      Biscoe                 0.964           -0.753                1.69 
## # ‚Ä¶ with 1 more variable: body_mass_g &lt;dbl[,1]&gt;
```

---
## Evaluating `nnet` Models (2)

- Manual accuracy calculation
- Nicer infrastructure might be available elsewhere?


```r
sum(pred_data$island == island_pred) / length(island_pred)
```

```
## [1] 0.6323529
```


---
class: inverse, middle
# Neural Networks in .bipsorange[tensorflow] and .bipsorange[keras]

---
## Example Task: MNIST

- MNIST handwritten image classification
- The "Hello World" of deep learning, apparently
- 3D arrays of 60k images, 28x28 greyscale matrices (0-255)

--


```r
library(keras)
mnist &lt;- dataset_mnist()
str(mnist)
```

```
## List of 2
##  $ train:List of 2
##   ..$ x: int [1:60000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 ...
##   ..$ y: int [1:60000(1d)] 5 0 4 1 9 2 1 3 1 4 ...
##  $ test :List of 2
##   ..$ x: int [1:10000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 ...
##   ..$ y: int [1:10000(1d)] 7 2 1 0 4 1 4 9 5 9 ...
```

---
## Example Digit

.pull-left[

```r
digit &lt;- mnist$train$x[1,,]
dim(digit)
```

```
## [1] 28 28
```


```r
plot(as.raster(digit, max = 255))
```
]

.pull-right[
&lt;img src="index_files/figure-html/unnamed-chunk-3-1.png" width="504" style="display: block; margin: auto;" /&gt;

]

---
## Preprocessing with `keras` (1)

- Unpacking data from nested list via `%&lt;-%` (`{zeallot}`)
- Reshaping via `keras::array_reshape` (it's a long story)
- Images are matrices with 28x28 = 784 columns

--


```r
c(c(train_images, train_labels), c(test_images, test_labels)) %&lt;-% mnist

train_images &lt;- array_reshape(train_images, c(60000, 28 * 28))
test_images &lt;- array_reshape(test_images, c(10000, 28 * 28))
```

---
## Preprocessing with `keras` (2)

- Scale images to 0-1
- Encode labels to categorical shapes (`keras::to_categorical`)
- Shuffle training data for good measure

--


```r
train_images &lt;- train_images / 255
test_images &lt;- test_images / 255

*train_labels &lt;- to_categorical(train_labels)
*test_labels &lt;- to_categorical(test_labels)

obs &lt;- nrow(train_images)
set.seed(123)
randomize &lt;- sample(seq_len(obs), size = obs, replace = FALSE)
train_images &lt;- train_images[randomize, ]
train_labels &lt;- train_labels[randomize, ]
```

---
## Build a Model in `keras`

- Define model either sequentially or with the functional API
- `keras_model_sequential` is the easiest way to get started
- `layer_dense` also defines activation function
- Note: No spaces in the `name` or you'll get unhelpful errors üôÉ


```r
# Number of input features
n_feat &lt;- ncol(train_images)

# Define model architecture sequentially
model &lt;- keras_model_sequential(name = "MNIST-MLP") %&gt;%
  layer_dense(units = 128, activation = "relu", input_shape = n_feat) %&gt;%
  layer_dense(units = 64, activation = "relu") %&gt;%
  layer_dense(units = 10, activation = "softmax", name = "Output")
```

---
## Compile a `keras`  Model

- Defines loss function, optimizer (with parameters), and other metrics
- Use either `"adam"` *or* `optimizer_adam(...)` and set parameters

--


```r
# Compile: Loss, optimizer, metrics
model %&gt;% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)
```

---
## Take a Look at a `keras` Model


```r
summary(model)
```

```
## Model: "MNIST-MLP"
## ________________________________________________________________________________
## Layer (type)                        Output Shape                    Param #     
## ================================================================================
## dense_1 (Dense)                     (None, 128)                     100480      
## ________________________________________________________________________________
## dense (Dense)                       (None, 64)                      8256        
## ________________________________________________________________________________
## Output (Dense)                      (None, 10)                      650         
## ================================================================================
## Total params: 109,386
## Trainable params: 109,386
## Non-trainable params: 0
## ________________________________________________________________________________
```

---
## Fit a `keras` Model

- Supply training images and labels (distinct, no formula interface)
- Set number of `epochs`, `batch_size`, ...
- Save training `history` for inspection

--


```r
history &lt;- model %&gt;% fit(
  x = train_images, y = train_labels,
  epochs = 5,             # Very short run
  batch_size = 64,        # Big batches for faster training
  validation_split = 0.2
)
```

---
## Look at the Model Fit

.pull-left[

Interactively, you'll get a live-updating plot üòé


```r
plot(history) +
  geom_path() +
  theme_minimal() +
  theme(legend.position = "top")
```

]

.pull-right[

&lt;img src="index_files/figure-html/keras-view-history2-1.png" width="504" style="display: block; margin: auto;" /&gt;

]

---
## Evaluate a `keras` Model

- Very simple to get relevant metrics


```r
model %&gt;% evaluate(test_images, test_labels, verbose = FALSE)
```

```
##       loss   accuracy 
## 0.08540654 0.97420001
```

- Further investigation via e.g. a confusion matrix


```r
predictions &lt;- model %&gt;% predict_classes(test_images)
actual &lt;- mnist$test$y
caret::confusionMatrix(factor(predictions), factor(actual))
```


---
class: inverse, middle
# Neural Networks in .bipsorange[torch] and .bipsorange[luz]

---
## Example Task: Cats and Dogs


```r
library(torch)
library(torchvision)
library(torchdatasets)
library(luz) 
```

- Using a kaggle dataset requires account + phone verification!


```r
ds &lt;- torchdatasets::dogs_vs_cats_dataset(
  root = "data",
  token = "~/.kaggle/kaggle.json",
  transform = . %&gt;%
    torchvision::transform_to_tensor() %&gt;%
    torchvision::transform_resize(size = c(224, 224)) %&gt;% 
    torchvision::transform_normalize(rep(0.5, 3), rep(0.5, 3)),
  target_transform = function(x) as.double(x) - 1
)
```

---
## Preprocessing in `torch`

- Shuffle training data, split train/test
- `dataset_subset` for syntax sugar
- `dataloader` created from data for efficiency


```r
train_ids &lt;- sample(1:length(ds), size = 0.6 * length(ds))
valid_ids &lt;- sample(setdiff(1:length(ds), train_ids), size = 0.2 * length(ds))
test_ids &lt;- setdiff(1:length(ds), union(train_ids, valid_ids))

*train_ds &lt;- dataset_subset(ds, indices = train_ids)
valid_ds &lt;- dataset_subset(ds, indices = valid_ids)
test_ds &lt;- dataset_subset(ds, indices = test_ids)

*train_dl &lt;- dataloader(train_ds, batch_size = 64, shuffle = TRUE, num_workers = 4)
valid_dl &lt;- dataloader(valid_ds, batch_size = 64, num_workers = 4)
test_dl &lt;- dataloader(test_ds, batch_size = 64, num_workers = 4)
```

---
## Build `torch` Model with `luz`

.pull-left[

- Network components are called `modules` in `{torch}`
- Plugging in a pre-trained model is easy(ish), where we disable gradients
- Sequential layers similar to `keras` are a `luz` bonus üòé

]

.pull-right[


```r
net &lt;- nn_module(
  initialize = function(output_size) {
    self$model &lt;- model_alexnet(pretrained = TRUE)
    
    for (par in self$parameters) par$requires_grad_(FALSE)
    
    self$model$classifier &lt;- nn_sequential(
      nn_dropout(0.5),
      nn_linear(9216, 512),
      nn_relu(),
      nn_linear(512, 256),
      nn_relu(),
      nn_linear(256, output_size)
    )
  },
  forward = function(x) {
    self$model(x)[,1]
  }
)
```

]

---
## Train `torch` Model with `luz`

- `luz::setup` is similar to `keras::compile`
- Both packages have a nice `fit` function


```r
fitted &lt;- net %&gt;%
  setup(
*   loss = nn_bce_with_logits_loss(),
*   optimizer = optim_adam,
    metrics = list(luz_metric_binary_accuracy_with_logits())
  ) %&gt;%
  set_hparams(output_size = 1) %&gt;% # param for initialize method
  set_opt_hparams(lr = 0.01) %&gt;% # params for optimizer (adam)
* fit(train_dl, epochs = 3, valid_data = valid_dl)
```




---
## Predicting with `torch`

- `predict` works as usual
- Sigmoid-transformation required to get probabilities





```r
preds &lt;- predict(fitted, test_dl)

probs &lt;- torch_sigmoid(preds)
print(probs, n = 5)
```


---
class: inverse, middle
# Takeaways

---
class: middle

## Long story short

- `nnet` is easy but very limited

--

    - Ideal for "I just want to train **some** neural nets"

--

- `{tensorflow}` and `{keras}` are established, very powerful, lots of examples available

--

    - The Python dependency can cause headaches 

--

- `{torch}` and `{luz}` are very young, but also very promising

--

    - No Python dependency, less (or less painful) headaches üôèüèª 

--

    - More assembly required than with `{keras}`, can be a good or a bad thing


---
class: middle

## References

- `{keras}` example: https://tensorflow.rstudio.com/guide/keras/examples/mnist_mlp/

    - All things `{keras}` at https://keras.rstudio.com/ (Documentation may lag behind Pythin version üòï)

- `{luz}` example: https://blogs.rstudio.com/ai/posts/2021-06-17-luz/

    - All things `{torch}` at https://mlverse.github.io/ (Very much in movement!)

---
class: center, middle, thanks
background-image: none

## Thank you for your attention

https://jemus42.github.io/talks/2021/nnpkgs/

.pull-left[.right[
.font150[**Contact**]  
.bipsblue[Lukas Burk]  
Leibniz Institute for Prevention Research and Epidemiology - BIPS GmbH  
Achterstra√üe 30  
D-28359 Bremen  
.bipsblue[burk@leibniz-bips.de]
]]
.pull-right[

&lt;img src="bips-logo.png" width="50%" style="display: block; margin: auto;" /&gt;

]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"slideNumberFormat": "%current%",
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
