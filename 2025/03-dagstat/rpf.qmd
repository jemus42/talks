---
title: Random Planted Forest  
subtitle: "A Directly Interpretable Tree Ensemble"
author:
  - Joseph T. Meyer\inst{5}
  - \textbf{Lukas Burk}\inst{1,2,3,4}
  - Munir Hiabu\inst{6}
  - Enno Mammen\inst{5}
#date: "2024-03-01"
format:
  beamer:
    template: template.tex
    widescreen: true # changes to sty now make this the only non-messed up option
    occasion: "DAGStat 2025 --- March 27th, 2025"
    incremental: true
    email: burk@leibniz-bips.de
    thankstext: "Thank you for your attention!"
    contactauthor: Lukas Burk
    institute:
      # Can't use \inst because title frame is made with tikz and \inst doesn't work within tikz nodes
      # and \insertinstitute did not work for some other godforsaken reason, idunno
        - \textsuperscript{1}Leibniz Institute for Prevention Research and Epidemiology -- BIPS
        - \textsuperscript{2}LMU Munich \quad \textsuperscript{3}University of Bremen
        - \textsuperscript{4}Munich Center for Machine Learning (MCML)
        - \textsuperscript{5}Heidelberg University \quad \textsuperscript{6}University of Copenhagen
    keep-tex: true
preview:
  port: 7777
editor_options: 
  chunk_output_type: console
filters:
   - latex-environment
environments: center
---

```{r setup, include=FALSE}
library(ggplot2)
library(patchwork)
library(randomPlantedForest)
library(glex)
library(kableExtra)

knitr::opts_chunk$set(
  fig.align = "center",
  dev = "ragg_png",
  dev.args = list(bg = "transparent")
)

theme_set(
  theme_minimal(base_size = 22) +
    theme(
      legend.position = "bottom",
      panel.spacing = unit(10, "mm"),
      plot.title.position = "plot"
    )
)

if (FALSE) {
  set.seed(2025)
  rpfit <- rpf(
    bikers ~ hr + temp + workingday + season + weathersit + windspeed + hum,
    data = bike,
    max_interaction = 3,
    ntrees = 50,
    splits = 100,
    t_try = 0.9,
    split_try = 5,
    nthreads = 10,
    purify = TRUE
  )
  rpglex <- glex(rpfit, bike)
  saveRDS(rpglex, here::here("2025/03-dagstat/rpglex.rds")) 

  predict(rpfit, bike) |>
    cbind(bikers = bike$bikers) |>
    dplyr::mutate(se = (.pred - bikers)^2) |>
    dplyr::summarise(rmse = sqrt(mean(se)))

  data.frame(.pred = predict(lmfit, bike), bikers = bike$bikers) |>
    dplyr::mutate(se = (.pred - bikers)^2) |>
    dplyr::summarise(rmse = sqrt(mean(se)))
}

rpglex <- readRDS(here::here("2025/03-dagstat/rpglex.rds"))

aggr <- readRDS(here::here("2025/03-dagstat/aggr.rds"))
scores <- readRDS(here::here("2025/03-dagstat/scores.rds"))

learner_cols <- c(
  "rpf" = "#F73098",
  "xgb" = "#3BA99C",
  "rpf_fixdepth" = "#CA1694",
  "xgb_fixdepth" = "#256A62",
  "ranger" = "#2171B5",
  "featureless" = "#484848"
)
learner_order <- names(learner_cols)
learner_label <- function(x) {
  x <- as.character(x)
  ret <- c("rpf" = "RPF", "xgb" = "XGBoost", "rpf_fixdepth" = "RPF (2)", "xgb_fixdepth" = "XGBoost (2)", "ranger" = "RF", "featureless" = "Featureless")[x]
  unname(ret)
}
learner_order_labelled <- learner_label(learner_order)

aggr[, learner_id := factor(learner_id, levels = rev(learner_order))]
scores[, learner_id := factor(learner_id, levels = rev(learner_order))]


```

## Motivation

-   Tree-based methods like Random Forest (RF):
    -   Fast & flexible
    -   Interpretable? → It depends
-   Wishlist:
    -   Meaningful *feature importance*
    -   Quantification of main- *and interaction* *effects*
-   Additive models useful for both

. . .

\vfill

::: center
→ **Random Planted Forest** (RPF): Additive Random Forest
:::

## Functional ANOVA Expansion

-   Regression with target $Y_i \in \mathbb{R}$, features $X_i \in \mathbb{R}^p$, instance $\mathbf{x}_i$
-   Expand prediction $\hat{y}_i = \hat{m}(\mathbf{x}_i)$ into
    -   $\hat{m}_{0}$: Average prediction (*"intercept"*)
    -   Terms $\hat{m}_S$ with feature $S \subseteq \{1, \ldots, p\}$

. . .

\vfill

\begin{align*}
\hat{m}(\mathbf{x}_i) = & \hat{m}_{0} + \\
&  \underbrace{\hat{m}_1(x_1) + \hat{m}_2(x_2) + \hat{m}_3(x_3)}_{\text{Main effect terms}} +
\end{align*}

\vfill

## Functional ANOVA Expansion

::: nonincremental
-   Regression with target $Y_i \in \mathbb{R}$, features $X_i \in \mathbb{R}^p$, instance $\mathbf{x}_i$
-   Expand prediction $\hat{y}_i = \hat{m}(\mathbf{x}_i)$ into
    -   $\hat{m}_{0}$: Average prediction (*"intercept"*)
    -   Terms $\hat{m}_S$ with feature $S \subseteq \{1, \ldots, p\}$

\vfill

\begin{align*}
\hat{m}(\mathbf{x}_i) = & \hat{m}_{0} + \\
&  \underbrace{\hat{m}_1(x_1) + \hat{m}_2(x_2) + \hat{m}_3(x_3)}_{\text{Main effect terms}} + \\
&  \underbrace{\hat{m}_{1,2}(x_1,x_2) + \hat{m}_{1,3}(x_1,x_3) + \hat{m}_{2,3}(x_2,x_3)}_{\text{2nd order interactions}} +
\end{align*}

\vfill
:::

## Functional ANOVA Expansion

::: nonincremental
-   Regression with target $Y_i \in \mathbb{R}$, features $X_i \in \mathbb{R}^p$, instance $\mathbf{x}_i$
-   Expand prediction $\hat{y}_i = \hat{m}(\mathbf{x}_i)$ into
    -   $\hat{m}_{0}$: Average prediction (*"intercept"*)
    -   Terms $\hat{m}_S$ with features $S \subseteq \{1, \ldots, p\}$

\vfill

\begin{align*}
\hat{m}(\mathbf{x}_i) = & \hat{m}_{0} + \\
&  \underbrace{\hat{m}_1(x_1) + \hat{m}_2(x_2) + \hat{m}_3(x_3)}_{\text{Main effect terms}} + \\
&  \underbrace{\hat{m}_{1,2}(x_1,x_2) + \hat{m}_{1,3}(x_1,x_3) + \hat{m}_{2,3}(x_2,x_3)}_{\text{2nd order interactions}} + \\ 
&  \underbrace{\hat{m}_{1,2,3}(x_1,x_2,x_3)}_{\text{3rd order interaction}}
\end{align*}

\vfill
:::

## Trees in Random Forest

![CARTlike](tree-cart.pdf){fig-align="center" height="75%"}

## Planted Trees (I)

![Planted](tree-planted-simple.pdf){#fig-planted fig-align="center" height="75%"}

## Planted Trees (II)

![Planted-large](tree-planted-large.pdf){fig-align="center" height="75%"}

## Key features of Random Planted Forests

-   Ensemble of trees like RF
-   Splits nodes multiple times (→ *non-binary* trees!)
-   Nodes keep track of features involved in construction
-   *Degree of interaction* can be constrained
-   Prediction built incrementally using residuals (cf. *Gradient Boosting*)
-   Tree stops after adjustable *number of splits*

## Application Example

-   `Bikeshare` regression dataset [^1]
-   Target `bikers`: Number of bikers per hour in 2011/2012
-   Focus on 3 features for illustration
    -   `hour` of day $\in \{0, 1, \ldots, 23\}$
    -   `temp` normalized temperature $\in [0, 1]$
    -   `workingday` $\in$ {`workingday`, `no workingday`}
-   Average prediction: $\hat{m}_0 \approx$ `r round(rpglex$intercept)`

## Main Effects

:::::: columns
::: {.column width="33%"}
```{r main-hr}
#| out-height: "70%"
#| fig-width: 6
plot_main_effect(rpglex, "hr") + theme_glex(base_size = 28)
```
:::

. . .

::: {.column width="33%"}
```{r main-temp}
#| out-height: "70%"
#| fig-width: 6
plot_main_effect(rpglex, "temp") + theme_glex(base_size = 28)
```
:::

. . .

::: {.column width="33%"}
```{r main-workingday}
#| out-height: "70%"
#| fig-width: 6
plot_main_effect(rpglex, "workingday") + 
  # labs(title = as.expression("hat(m)[1]")) +
  theme_glex(base_size = 28)
```
:::
::::::

\begin{center}

$\hat{m} = \hat{m}_0 + \hat{m}_{\texttt{hr}}(\texttt{hr}) + \hat{m}_{\texttt{temp}}(\texttt{temp}) + \hat{m}_{\texttt{workingday}}(\texttt{workingday}) + \ldots$

\end{center}

## Hour $\times$ Working Day: "Rush Hour Effect"

```{r twoway-hr-workingday}
#| out-height: "67%"
#| fig-width: 12
#| fig-height: 4

plot_twoway_effects(rpglex, c("hr", "workingday")) + 
  scale_y_continuous(breaks = seq(-200, 200, 50)) +
  labs(color = NULL) + 
  theme_glex(base_size = 28) +
  theme(legend.position = "right")
```

\begin{center}

$\ldots + \hat{m}_{\texttt{hr}, \texttt{workingday}}(\texttt{hr}, \texttt{workingday}) + \ldots$

\end{center}

## More 2nd Order Interactions

::::: columns
::: {.column width="50%" align="center"}
```{r twoway-temp-workingday}
#| out-height: "63%"
#| fig-height: 6
#| fig-width: 9
plot_twoway_effects(rpglex, c("temp", "workingday")) + 
  labs(color = NULL)  + 
  theme_glex(base_size = 28)
```

$+ \hat{m}_{\texttt{temp}, \texttt{workingday}}(\texttt{temp}, \texttt{workingday})$
:::

. . .

::: {.column width="50%" align="center"}
```{r twoway-hr-temp}
#| out-height: "63%"
#| fig-height: 6
#| fig-width: 9
plot_twoway_effects(rpglex, c("hr", "temp"))  + 
  #labs(color = NULL) +
  guides(color = guide_colorbar(title.position = "left", title.hjust = 0.5)) +
  theme_glex(base_size = 28) +
  theme(legend.key.width = unit("60", units = "pt"))
```

$+ \hat{m}_{\texttt{hr}, \texttt{temp}}(\texttt{hr}, \texttt{temp}) + \ldots$
:::
:::::

## 3rd Order Interaction

```{r threeway-hr-temp-woringday}
#| out-height: "77.5%"
#| fig-width: 14
plot_threeway_effects(rpglex, c("hr", "temp", "workingday")) + 
  #labs(color = NULL) +
  guides(color = guide_colorbar(title.position = "left", title.hjust = 0.5)) +
  theme_glex(
    base_size = 28
  ) +
  theme(legend.key.width = unit("60", units = "pt"))
```

<!-- $\ldots + \hat{m}_{\texttt{hr}, \texttt{temp}, \texttt{workingday}}(\texttt{hr}, \texttt{temp}, \texttt{workingday})$ -->

## Feature Importance in RPF

$$\mathrm{FI}_S = \frac{1}{n} \sum_{i=1}^n |\hat{m}_S(\mathbf{x}_i)|$$

-   Average of absolute terms $\hat{m}_S$ for $S$ of interest
-   Scores also *per interaction* term
-   Importance scores on *same scale* as prediction

````{=html}
<!--

## Feature Importance: Main Terms

```{r vi-plot-main}
#| fig-width: 8
#| fig-height: 3
#| out-height: "72%"
vi_rpf <- glex_vi(rpglex)

keep_vis <- sapply(c("hr", "temp", "workingday"), \(term) {
  glex:::find_term_matches(term, vi_rpf$term)
}) |> unlist() |> unique()

drop_vis <- sapply(c("season", "weathersit", "windspeed", "hum"), \(term) {
  glex:::find_term_matches(term, vi_rpf$term)
}) |> unlist() |> unique()


vi_rpf_subset <- vi_rpf[keep_vis, ]
vi_rpf_subset_neg <- vi_rpf[-drop_vis]

autoplot(vi_rpf_subset[degree == 1]) + 
  labs(title = NULL) + 
  theme_glex(base_size = 18, grid_x = FALSE, grid_y = TRUE) +
  theme(
    #axis.text.y = element_text(size = rel(1.25)),
    legend.position = "none"
  )
```

-->
````

## Feature Importance: All Terms

```{r vi-plot-thresh}
#| out-height: "72%"
#| fig-height: 5
#| fig-width: 9
vi_rpf_subset_neg |>
  autoplot() + 
    labs(title = NULL) +
    theme_glex(base_size = 18, grid_x = FALSE, grid_y = TRUE) +
    theme(
      axis.text.y = element_text(size = 22)
      # axis.title.x = element_text(size = 20),
      # legend.title = element_text(size = 18)
    )

```

````{=html}
<!--

## Feature Importance by Order of Interaction

```{r vi-plot-degree}
#| out-height: "72%"
#| fig-height: 4
autoplot(vi_rpf, by_degree = TRUE) + 
  scale_x_continuous(breaks = seq(0, 200, 25)) +
  labs(title = NULL, y = "Order of Interaction") +
  theme_glex(base_size = 18, grid_x = FALSE, grid_y = TRUE)
```

-->
````

## No Free Lunch

::: center
(↑) Gains in interpretability $\Rightarrow$ (↓) sacrifices in predictive performance?
:::

\vfill

. . .

-   Benchmark on **28** datasets [^2] comparing RPF with *XGBoost* & *RF*
-   → RPF never best, rarely bad, usually close to XGBoost
-   RPF slower (especially with large data)


<!--

## Benchmark Results (Aggregated)

::: nonincremental

```{r bm-aggr-rrse}
#| out-height: "70%"
#| fig-height: 3
aggr |>
    dplyr::filter(learner_id != "featureless") |>
    ggplot(aes(y = learner_id, x = 100 * rrse, fill = learner_id)) +
    geom_boxplot(alpha = .5) +
    geom_point(
      position = position_jitterdodge(dodge.width = .5),
      shape = 21, stroke = .1, size = 2
    ) +
    scale_x_continuous(
      # guide = guide_axis(position = "top"), 
      limits = c(0, 100)
    ) +
    scale_y_discrete(labels = learner_label) +
    scale_fill_manual(values = learner_cols, guide = "none") +
    labs(
      #title = "Aggregated scores over all tasks",
      x = "RRSE (%) (lower is better)", y = NULL
    ) +
    theme_minimal(base_size = 20) +
    theme(
      plot.title.position = "plot"
    )
```

$\mathrm{RRSE} := \sqrt{\mathrm{SSE}(Y, \hat{Y})\ /\ \mathrm{SSE}(Y, \bar{Y})}$

:::

-->


## Benchmark Results (Selected Tasks)

```{r bm-scores-sel}
#| out-height: "75%"
#| fig-height: 3
#| fig-width: 9
# out-width: "70%"

p <- lapply(c("kin8nm", "miami_housing"), \(x) {
scores |>
    dplyr::filter(task_id %in% c(x
      # #"space_ga", 
      # "kin8nm", 
      # #"white_wine",
      # "miami_housing" 
    )) |>
    dplyr::filter(learner_id != "featureless") |>
    dplyr::mutate(
      task_label = stringr::str_replace(task_label, "\\s\\(", "\\\n("),
      task_label = reorder(task_label, n*p)
      #task_label = stringr::str_wrap(task_label, width = 20, whitespace_only = FALSE)
    ) |>
    ggplot(aes(y = learner_id, x = rmse, fill = learner_id, color = learner_id)) +
    facet_wrap(vars(task_label), ncol = 1, scales = "free") +
    geom_boxplot(alpha = .5) +
    geom_jitter(shape = 21, stroke = .5, alpha = .15, size = 1) +
    scale_fill_manual(values = learner_cols, guide = "none", aesthetics = c("color", "fill")) +
    scale_y_discrete(labels = learner_label) +
    labs(
      # title = "Scores per task",
      # subtitle = "Tasks ordered by n * p, decreasing",
      y = NULL, x = "RMSE"
    ) +
    theme_minimal(base_size = 16) +
    theme(
      panel.spacing.x = unit(1, "cm"),
      plot.title.position = "plot"
    )
})


```

::::: columns
::: {.column width="50%"}
```{r bm-scores-sel-1}
#| out-height: "75%"
#| fig-height: 4
#| fig-width: 5
# out-width: "70%"

p[[1]]

```
:::

. . .

::: {.column width="50%"}
```{r bm-scores-sel-2}
#| out-height: "75%"
#| fig-height: 4
#| fig-width: 5
# out-width: "70%"

p[[2]]

```
:::
:::::

<!-- $\mathrm{RRSE} := \sqrt{\mathrm{SSE}(Y, \hat{Y})\ /\ \mathrm{SSE}(Y, \bar{Y})}$ -->

## Summary

::: center
**Random Planted Forests**: Additive Random Forests
:::

. . .

\vfill

-   (↑) Feature *importance* on same scale as target
-   (↑) *Main-* and *interaction effects*
-   (↑) R package available [^3]
-   (→) Competetive predictive performance (mostly)
-   (↓) Slower for large data (Optimization WIP!)
-   Related work: `glex` [^4]: Same decomposition but post-hoc for XGBoost & RF


[^1]: [UCI ML repository](https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset)

[^2]: Fischer et al. (2023): OpenML-CTR23

[^3]: [github.com/PlantedML/randomPlantedForest](https://github.com/PlantedML/randomPlantedForest)

[^4]: Hiabu et al. (2023): Unifying local and global model explanations [...]
