---
title: |
  Random Planted Forest\
  A Directly Interpretable Tree Ensemble
subtitle: "Awwww yeah"
author:
  - Meyer, J. T.\inst{5}
  - Burk, L.\inst{1,2,3,4}
  - Hiabu, M.\inst{6}
  - Mammen, E.\inst{5}
#date: "2024-03-01"
format:
  beamer:
    template: template.tex
    widescreen: true # changes to sty now make this the only non-messed up option
    occasion: "DAGStat 2025 --- March 27th, 2025"
    incremental: true
    email: burk@leibniz-bips.de
    thankstext: "Thank you for your attention!"
    contactauthor: Lukas Burk
    institute:
      # Can't use \inst because title frame is made with tikz and \inst doesn't work within tikz nodes
      # and \insertinstitute did not work for some other godforsaken reason, idunno
        - \textsuperscript{1}Leibniz Institute for Prevention Research and Epidemiology -- BIPS
        - \textsuperscript{2}LMU Munich \quad \textsuperscript{3}University of Bremen
        - \textsuperscript{4}Munich Center for Machine Learning (MCML)
        - \textsuperscript{5}Heidelberg University \quad \textsuperscript{6}University of Copenhagen
    # bibliography:
    #   - references.bib
    cite-method: biblatex
    keep-tex: true
preview:
  port: 7777
editor_options: 
  chunk_output_type: console
# pdf-engine: xelatex
bibliography: references.bib
---

```{r setup, include=FALSE}
library(ggplot2)
library(patchwork)
library(randomPlantedForest)
library(glex)
library(kableExtra)

knitr::opts_chunk$set(
  fig.align = "center",
  dev = "ragg_png",
  dev.args = list(bg = "transparent")
)

theme_set(
  theme_minimal(base_size = 22) +
    theme(
      legend.position = "bottom",
      panel.spacing = unit(10, "mm"),
      plot.title.position = "plot"
    )
)

if (FALSE) {
  set.seed(2025)
  rpfit <- rpf(
    bikers ~ hr + temp + workingday + season + weathersit + windspeed + hum,
    data = bike,
    max_interaction = 3,
    ntrees = 50,
    splits = 100,
    t_try = 0.9,
    split_try = 5,
    nthreads = 10
  )
  rpglex <- glex(rpfit, bike)
  saveRDS(rpglex, here::here("2025/03-dagstat/rpglex.rds")) 

  predict(rpfit, bike) |>
    cbind(bikers = bike$bikers) |>
    dplyr::mutate(se = (.pred - bikers)^2) |>
    dplyr::summarise(rmse = sqrt(mean(se)))

  data.frame(.pred = predict(lmfit, bike), bikers = bike$bikers) |>
    dplyr::mutate(se = (.pred - bikers)^2) |>
    dplyr::summarise(rmse = sqrt(mean(se)))
}

rpglex <- readRDS(here::here("2025/03-dagstat/rpglex.rds"))

aggr <- readRDS(here::here("2025/03-dagstat/aggr.rds"))
scores <- readRDS(here::here("2025/03-dagstat/scores.rds"))

learner_cols <- c(
  "rpf" = "#F73098",
  "xgb" = "#3BA99C",
  "rpf_fixdepth" = "#CA1694",
  "xgb_fixdepth" = "#256A62",
  "ranger" = "#2171B5",
  "featureless" = "#484848"
)
learner_order <- names(learner_cols)
learner_label <- function(x) {
  x <- as.character(x)
  ret <- c("rpf" = "RPF", "xgb" = "XGBoost", "rpf_fixdepth" = "RPF (2)", "xgb_fixdepth" = "XGBoost (2)", "ranger" = "RF", "featureless" = "Featureless")[x]
  unname(ret)
}
learner_order_labelled <- learner_label(learner_order)

aggr[, learner_id := factor(learner_id, levels = rev(learner_order))]
scores[, learner_id := factor(learner_id, levels = rev(learner_order))]


```

## Motivation

-   Individual decision trees: Easy to interpret
-   Random Forest (RF): Less so
-   **Desirable properties**: 
    -   Meaningful feature importance rather than rankings
    -   Quantification of main- _and interaction_ effects
-   Additive models (LM, GAM, ...) can provide both
-   → Need to manually specify interactions in model fit

**Random Planted Forest** (RPF): Additive Random Forest


## Functional ANOVA Expansion

- Setting: Regression with target $Y_i \in \mathbb{R}^p$ and feature vector $\mathbf{x_i}$
-   Decompose prediction $\mathbb{E}(Y_i | X = \mathbf{x}_i) = \hat{m}(\mathbf{x_i})$ into 
    -   Average prediction $\hat{m}_{0}$ ("intercept")
    -   Terms $\hat{m}_S$ with $S \subseteq \{1, \ldots, s\}$

. . .

\begin{align*}
\hat{m}(\mathbf{x}) & = \hat{m}_{0} \\
& + \underbrace{\hat{m}_1(x_1) + \hat{m}_2(x_2) + \hat{m}_3(x_3)}_{\text{Main effects}} \\
& + \underbrace{\hat{m}_{1,2}(x_1,x_2) + \hat{m}_{1,3}(x_1,x_3) + \hat{m}_{2,3}(x_2,x_3)}_{\text{2nd order interactions}} \\ 
& + \underbrace{\hat{m}_{1,2,3}(x_1,x_2,x_3)}_{\text{3rd order interaction}}
\end{align*}

## Trees in Random Forest

![CARTlike](tree-cart.pdf){fig-align="center" height=75%}

## Planted Trees

![Planted](tree-planted-simple.pdf){#fig-planted fig-align="center" height=75%}

## Planted Trees

![Planted-large](tree-planted-large.pdf){#fig-planted fig-align="center" height=75%}



## Key Differences of RPF to RF

- Splits some nodes multiple times (→ non-binary trees)
- Keeps track of features involved in split
- Degree of interaction can be constrained
- Stopping after adjustable number of splits
- Prediction is average of additive $\hat{m}_S$ estimates


## Application Example

-   `Bikeshare` regression dataset [^1]
-   Target `bikers`: Number of bikers on a given day in 2011/2012
-   Focus on 3 features for example
    -   `hour` of day $\in \{0, 1, \ldots, 23\}$
    -   `temp` normalized temperature $\in [0, 1]$
    -   `workingday` binary → \{`workingday`, `no workingday`\}
- Average prediction: $\hat{m}_0 \approx$ `r round(rpglex$intercept, 1)`

[^1]: from [UCI ML repository](https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset)

## Main Effects

:::: {.columns}

::: {.column width="33%"}

```{r main-hr}
#| out-height: "75%"
#| fig-width: 6
plot_main_effect(rpglex, "hr") + theme_glex(base_size = 24)
```

:::

. . .

::: {.column width="33%"}

```{r main-temp}
#| out-height: "75%"
#| fig-width: 6
plot_main_effect(rpglex, "temp") + theme_glex(base_size = 24)
```

:::

. . .

::: {.column width="33%"}

```{r main-workingday}
#| out-height: "75%"
#| fig-width: 6
plot_main_effect(rpglex, "workingday") + theme_glex(base_size = 24)
```

:::

::::


## Hour $\times$ Working Day: "Rush Hour Effect"

```{r twoway-hr-workingday} 
#| out-height: "75%"

plot_twoway_effects(rpglex, c("hr", "workingday")) + 
  scale_y_continuous(breaks = seq(-200, 200, 50)) +
  labs(color = NULL)
```

## More 2nd Order Interactions

:::: {.columns}

::: {.column width="50%"}

```{r}
#| out-height: "75%"
plot_twoway_effects(rpglex, c("temp", "workingday")) + labs(color = NULL)
```

:::

. . .

::: {.column width="50%"}

```{r}
#| out-height: "75%"
plot_twoway_effects(rpglex, c("hr", "temp"))
```

:::

::::


## 3rd Order Interaction

```{r}
#| out-height: "75%"

plot_threeway_effects(rpglex, c("hr", "temp", "workingday"))
```

## Feature Importance

-   Average of absolute values of term of interest
  
. . .

$$\mathrm{VI}_S(X) = \frac{1}{n} \sum_{i=1}^n |\hat{m}_S(\mathbf{x}_i)|$$

-   Unlike RF Feature importance:
    -   Scores _per interaction_ term
    -   Importance scores on same scale as prediction


## Feature Importance per Main Term

```{r vi-plot}
#| fig-width: 8
#| fig-height: 4
#| out-height: "75%"
vi_rpf <- glex_vi(rpglex)

keep_vis <- sapply(c("hr", "temp", "workingday"), \(term) {
  glex:::find_term_matches(term, vi_rpf$term)
}) |> unlist() |> unique()


vi_rpf_subset <- vi_rpf[keep_vis, ]

autoplot(vi_rpf_subset[degree == 1]) + 
  labs(title = NULL) + 
  theme(
    axis.text.y = element_text(size = rel(1.25)),
    legend.position = "none"
  )
```


## Feature Importance for All Terms

```{r vi-plot-thresh}
#| out-height: "75%"
#| fig-height: 4
autoplot(vi_rpf_subset, threshold = 2) + 
  labs(title = NULL, caption = "Values <2 aggregated under 'Remaining terms'") +
  theme(
    axis.text.y = element_text(size = rel(1.25))
  )
```

## Feature Importance by Order of Interaction

```{r vi-plot-degree}
#| out-height: "75%"
#| fig-height: 4
autoplot(vi_rpf, by_degree = TRUE) + 
  labs(title = NULL, y = "Order of Interaction") +
  theme(
    axis.text.y = element_text(size = rel(1.25))
  )
```

## No Free Lunch

Better interpretibility → worse predictive performance?

. . .

-   Benchmark comparing RPF with XGBoost, RF incl. tuning
-   28 datasets from OpenML-CTR23 regression benchmark suite [^ctr23]
-   Also comparing XGBoost / RPF with interactions restrained to 2
-   Generally XGBoost best, RPF and RF closely behind

[^ctr23]: @fischer2023openmlctr

## Benchmark Results (Aggregated)

::: nonincremental

```{r bm-aggr-rrse}
#| out-height: "70%"
#| fig-height: 3
aggr |>
    dplyr::filter(learner_id != "featureless") |>
    ggplot(aes(y = learner_id, x = 100 * rrse, fill = learner_id)) +
    geom_boxplot(alpha = .5) +
    geom_point(
      position = position_jitterdodge(dodge.width = .5),
      shape = 21, stroke = .1, size = 2
    ) +
    scale_x_continuous(
      # guide = guide_axis(position = "top"), 
      limits = c(0, 100)
    ) +
    scale_y_discrete(labels = learner_label) +
    scale_fill_manual(values = learner_cols, guide = "none") +
    labs(
      #title = "Aggregated scores over all tasks",
      x = "RRSE (%)", y = NULL
    ) +
    theme_minimal(base_size = 14) +
    theme(
      plot.title.position = "plot"
    )
```

Root-Relative Squared Error (RRSE) $\sqrt{\tfrac{SSE(Y, \hat{Y})}{SSE(Y, \bar{Y})}}$\\
→ Featureless model scores 1, perfect score 0

:::

## Results for Selected Tasks

```{r bm-scores-sel}
#| out-height: "75%"
#| out-width: "70%"
#| fig-height: 5
scores |>
    dplyr::filter(task_id %in% c("space_ga", "kin8nm", "miami_housing", "white_wine")) |>
    dplyr::filter(learner_id != "featureless") |>
    dplyr::mutate(
      task_label = stringr::str_replace(task_label, "\\s\\(", "\\\n("),
      task_label = reorder(task_label, n*p)
      #task_label = stringr::str_wrap(task_label, width = 20, whitespace_only = FALSE)
    ) |>
    ggplot(aes(y = learner_id, x = 100 * rrse, fill = learner_id, color = learner_id)) +
    facet_wrap(vars(task_label), ncol = 2, scales = "free") +
    geom_boxplot(alpha = .5) +
    geom_jitter(shape = 21, stroke = .5, alpha = .15, size = 1) +
    scale_fill_manual(values = learner_cols, guide = "none", aesthetics = c("color", "fill")) +
    scale_y_discrete(labels = learner_label) +
    labs(
      # title = "Scores per task",
      # subtitle = "Tasks ordered by n * p, decreasing",
      y = NULL, x = "RRSE (%)"
    ) +
    theme_minimal(base_size = 13) +
    theme(
      panel.spacing.x = unit(1, "cm"),
      plot.title.position = "plot"
    )
```

## Related work

- Glex[^glex]: Same ANOVA decomposition but generally for tree-based methods (e.g. XGBoost)
- **Idea**: 
  - Fit XGBoost with more shallow trees
  - Extract component-wise predictions from tree structure
- **Benefit**: Use existing / well known / well optimzied algorithm
- **Drawback**: Computationally intensive post-hoc computation

[^glex]: @hiabu2023glex / [github.com/PlantedML/glex](https://github.com/PlantedML/glex)

## Summary

**Random Planted Forests** = Additive Random Forests

- (+) Interpretability on global and local perspective
- (+) Interpretable on same scale as target
- (~) Predictive performance worse but similar to comparable algorithms
- (-) Computationally heavy for large data
- (+) R package available [^rpfgh]


[^rpfgh]: [github.com/PlantedML/randomPlantedForest](https://github.com/PlantedML/randomPlantedForest)
