---
title: |
  Random Planted Forest\
  A Directly Interpretable Tree Ensemble
subtitle: "Awwww yeah"
author:
  - Meyer, J. T.\inst{5}
  - Burk, L.\inst{1,2,3,4}
  - Hiabu, M.\inst{6}
  - Mammen, E.\inst{5}
#date: "2024-03-01"
format:
  beamer:
    template: template.tex
    widescreen: true # changes to sty now make this the only non-messed up option
    occasion: "DAGStat 2025 --- March 27th, 2025"
    incremental: true
    email: burk@leibniz-bips.de
    thankstext: "Thank you for your attention!"
    contactauthor: Lukas Burk
    institute:
      # Can't use \inst because title frame is made with tikz and \inst doesn't work within tikz nodes
      # and \insertinstitute did not work for some other godforsaken reason, idunno
        - \textsuperscript{1}Leibniz Institute for Prevention Research and Epidemiology -- BIPS
        - \textsuperscript{2}LMU Munich \quad \textsuperscript{3}University of Bremen
        - \textsuperscript{4}Munich Center for Machine Learning (MCML)
        - \textsuperscript{5}Heidelberg University \quad \textsuperscript{6}University of Copenhagen
    # bibliography:
    #   - references.bib
    cite-method: biblatex
    keep-tex: true
preview:
  port: 7777
editor_options: 
  chunk_output_type: console
# pdf-engine: xelatex
bibliography: references.bib
---

```{r setup, include=FALSE}
library(ggplot2)
library(patchwork)
library(randomPlantedForest)
library(glex)
library(kableExtra)

knitr::opts_chunk$set(
  fig.align = "center",
  dev = "ragg_png",
  dev.args = list(bg = "transparent")
)

theme_set(
  theme_minimal(base_size = 22) +
    theme(
      legend.position = "bottom",
      panel.spacing = unit(10, "mm"),
      plot.title.position = "plot"
    )
)

if (FALSE) {
  set.seed(2025)
  rpfit <- rpf(
    bikers ~ hr + temp + workingday + season + weathersit + windspeed + hum,
    data = bike,
    max_interaction = 3,
    ntrees = 50,
    splits = 100,
    t_try = 0.9,
    split_try = 5,
    nthreads = 10
  )
  rpglex <- glex(rpfit, bike)
  saveRDS(rpglex, here::here("2025/03-dagstat/rpglex.rds")) 

  predict(rpfit, bike) |>
    cbind(bikers = bike$bikers) |>
    dplyr::mutate(se = (.pred - bikers)^2) |>
    dplyr::summarise(rmse = sqrt(mean(se)))

  data.frame(.pred = predict(lmfit, bike), bikers = bike$bikers) |>
    dplyr::mutate(se = (.pred - bikers)^2) |>
    dplyr::summarise(rmse = sqrt(mean(se)))
}

rpglex <- readRDS(here::here("2025/03-dagstat/rpglex.rds"))

aggr <- readRDS(here::here("2025/03-dagstat/aggr.rds"))
scores <- readRDS(here::here("2025/03-dagstat/scores.rds"))

learner_cols <- c(
  "rpf" = "#F73098",
  "xgb" = "#3BA99C",
  "rpf_fixdepth" = "#CA1694",
  "xgb_fixdepth" = "#256A62",
  "ranger" = "#2171B5",
  "featureless" = "#484848"
)
learner_order <- names(learner_cols)
learner_label <- function(x) {
  x <- as.character(x)
  ret <- c("rpf" = "RPF", "xgb" = "XGBoost", "rpf_fixdepth" = "RPF (2)", "xgb_fixdepth" = "XGBoost (2)", "ranger" = "RF", "featureless" = "Featureless")[x]
  unname(ret)
}
learner_order_labelled <- learner_label(learner_order)

aggr[, learner_id := factor(learner_id, levels = rev(learner_order))]
scores[, learner_id := factor(learner_id, levels = rev(learner_order))]


```

## Motivation

-   Want: Fast, flexible, and **interpretable** predictive models
-   Tree-based methods like Random Forest (RF):
    -   Fast & flexible
    -   Interpretable? → It depends
-   Desirable properties: 
    -   Meaningful _feature importance_
    -   Quantification of main- _and interaction_ _effects_
-   Additive models (LM, GAM, ...) can provide both

. . .

\vfill

\begin{center}
→ \textbf{andom Planted Forest} (RPF): Additive Random Forest
\end{center}

## Functional ANOVA Expansion

- Setting: Regression with target $Y_i \in \mathbb{R}$, features $X_i \in \mathbb{R}^p$, instance $\mathbf{x}_i$
- Expand prediction $\mathbb{E}(Y_i | X = \mathbf{x}_i) = \hat{m}(\mathbf{x}_i)$ into 
    -   $\hat{m}_{0}$: Average prediction (_"intercept"_) 
    -   Terms $\hat{m}_S$ with feature set $S \subseteq \{1, \ldots, p\}$

. . .

\begin{align*}
\hat{m}(\mathbf{x}_i) = & \hat{m}_{0} + \\
&  \underbrace{\hat{m}_1(x_1) + \hat{m}_2(x_2) + \hat{m}_3(x_3)}_{\text{Main effect terms}} + \\
&  \underbrace{\hat{m}_{1,2}(x_1,x_2) + \hat{m}_{1,3}(x_1,x_3) + \hat{m}_{2,3}(x_2,x_3)}_{\text{2nd order interactions}} + \\ 
&  \underbrace{\hat{m}_{1,2,3}(x_1,x_2,x_3)}_{\text{3rd order interaction}}
\end{align*}

## Trees in Random Forest

![CARTlike](tree-cart.pdf){fig-align="center" height=75%}

## Planted Trees (I)

![Planted](tree-planted-simple.pdf){#fig-planted fig-align="center" height=75%}

## Planted Trees (II)

![Planted-large](tree-planted-large.pdf){#fig-planted fig-align="center" height=75%}

## Key features of Random Planted Forests

- Ensemble of trees like RF, different structure
- Splits nodes multiple times (→ _non-binary_ trees!)
- Nodes keep track of features involved in construction
- _Degree of interaction_ can be constrained
- Tree stops after adjustable _number of splits_
- Prediction built up incrementally using residuals (cf. Gradient Boosting)

## Application Example

-   `Bikeshare` regression dataset [^1]
-   Target `bikers`: Number of bikers on a given day in 2011/2012
-   Focus on 3 features for example
    -   `hour` of day $\in \{0, 1, \ldots, 23\}$
    -   `temp` normalized temperature $\in [0, 1]$
    -   `workingday` binary → \{`workingday`, `no workingday`\}
- Average prediction: $\hat{m}_0 \approx$ `r round(rpglex$intercept)`

[^1]: from [UCI ML repository](https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset)

## Main Effects

:::: {.columns}

::: {.column width="33%"}


```{r main-hr}
#| out-height: "70%"
#| fig-width: 6
plot_main_effect(rpglex, "hr") + theme_glex(base_size = 28)
```

:::

. . .

::: {.column width="33%"}


```{r main-temp}
#| out-height: "70%"
#| fig-width: 6
plot_main_effect(rpglex, "temp") + theme_glex(base_size = 28)
```

:::

. . .

::: {.column width="33%"}


```{r main-workingday}
#| out-height: "70%"
#| fig-width: 6
plot_main_effect(rpglex, "workingday") + 
  # labs(title = as.expression("hat(m)[1]")) +
  theme_glex(base_size = 28)
```

:::

::::

. . .

\begin{center}

$\hat{m} = \hat{m}_0 + \hat{m}_{\texttt{hr}}(\texttt{hr}) + \hat{m}_{\texttt{temp}}(\texttt{temp}) + \hat{m}_{\texttt{workingday}}(\texttt{workingday}) + \ldots$

\end{center}

## Hour $\times$ Working Day: "Rush Hour Effect"

```{r twoway-hr-workingday} 
#| out-height: "67%"
#| fig-width: 12
#| fig-height: 4

plot_twoway_effects(rpglex, c("hr", "workingday")) + 
  scale_y_continuous(breaks = seq(-200, 200, 50)) +
  labs(color = NULL) + 
  theme_glex(base_size = 28) +
  theme(legend.position = "right")
```

\begin{center}

$\ldots + \hat{m}_{\texttt{hr}, \texttt{workingday}}(\texttt{hr}, \texttt{workingday}) + \ldots$

\end{center}

## More 2nd Order Interactions

:::: {.columns}

::: {.column width="50%" align=center}

```{r twoway-temp-workingday}
#| out-height: "63%"
#| fig-height: 6
#| fig-width: 9
plot_twoway_effects(rpglex, c("temp", "workingday")) + 
  labs(color = NULL)  + 
  theme_glex(base_size = 28)
```

$+ \hat{m}_{\texttt{temp}, \texttt{workingday}}(\texttt{temp}, \texttt{workingday})$

:::

. . .

::: {.column width="50%" align=center}

```{r twoway-hr-temp}
#| out-height: "63%"
#| fig-height: 6
#| fig-width: 9
plot_twoway_effects(rpglex, c("hr", "temp"))  + 
  #labs(color = NULL) +
  guides(color = guide_colorbar(title.position = "left", title.hjust = 0.5)) +
  theme_glex(base_size = 28) +
  theme(legend.key.width = unit("60", units = "pt"))
```

$+ \hat{m}_{\texttt{hr}, \texttt{temp}}(\texttt{hr}, \texttt{temp}) + \ldots$


:::

::::


## 3rd Order Interaction

```{r threeway-hr-temp-woringday}
#| out-height: "77.5%"
#| fig-width: 14
plot_threeway_effects(rpglex, c("hr", "temp", "workingday")) + 
  #labs(color = NULL) +
  guides(color = guide_colorbar(title.position = "left", title.hjust = 0.5)) +
  theme_glex(
    base_size = 28
  ) +
  theme(legend.key.width = unit("60", units = "pt"))
```

<!-- $\ldots + \hat{m}_{\texttt{hr}, \texttt{temp}, \texttt{workingday}}(\texttt{hr}, \texttt{temp}, \texttt{workingday})$ -->

## Feature Importance in RPF

-   Average of absolute values of term $\hat{m}_S$ of interest

$$\mathrm{FI}_S = \frac{1}{n} \sum_{i=1}^n |\hat{m}_S(\mathbf{x}_i)|$$

-   Unlike RF Feature importance:
    -   Scores also _per interaction_ term
    -   Importance scores on _same scale_ as prediction


## Feature Importance: Main Terms

```{r vi-plot-main}
#| fig-width: 8
#| fig-height: 3
#| out-height: "72%"
vi_rpf <- glex_vi(rpglex)

keep_vis <- sapply(c("hr", "temp", "workingday"), \(term) {
  glex:::find_term_matches(term, vi_rpf$term)
}) |> unlist() |> unique()

drop_vis <- sapply(c("season", "weathersit", "windspeed", "hum"), \(term) {
  glex:::find_term_matches(term, vi_rpf$term)
}) |> unlist() |> unique()


vi_rpf_subset <- vi_rpf[keep_vis, ]
vi_rpf_subset_neg <- vi_rpf[-drop_vis]

autoplot(vi_rpf_subset[degree == 1]) + 
  labs(title = NULL) + 
  theme_glex(base_size = 20, grid_x = FALSE, grid_y = TRUE) +
  theme(
    #axis.text.y = element_text(size = rel(1.25)),
    legend.position = "none"
  )
```


## Feature Importance: All Terms

```{r vi-plot-thresh}
#| out-height: "72%"
#| fig-height: 5
vi_rpf_subset_neg |>
  autoplot() + 
    labs(title = NULL) +
    theme_glex(base_size = 22, grid_x = FALSE, grid_y = TRUE)
```

<!--

## Feature Importance by Order of Interaction

```{r vi-plot-degree}
#| out-height: "72%"
#| fig-height: 4
autoplot(vi_rpf, by_degree = TRUE) + 
  scale_x_continuous(breaks = seq(0, 200, 25)) +
  labs(title = NULL, y = "Order of Interaction") +
  theme_glex(base_size = 18, grid_x = FALSE, grid_y = TRUE)
```

-->

## No Free Lunch

Gains in interpretibility → sacrifices in predictive performance?

\vfill

. . .

-   Benchmark on 28 datasets [^ctr23] comparing RPF with XGBoost & RF, incl. tuning
-   Additionally comparing XGBoost & RPF with constrained order of interaction to _2_

. . .

\vfill

→ Generally: RPF never best, rarely bad, usually close

[^ctr23]: OpenML-CTR23 regression benchmark suite: @fischer2023openmlctr

## Benchmark Results (Aggregated)

::: nonincremental

```{r bm-aggr-rrse}
#| out-height: "70%"
#| fig-height: 3
aggr |>
    dplyr::filter(learner_id != "featureless") |>
    ggplot(aes(y = learner_id, x = 100 * rrse, fill = learner_id)) +
    geom_boxplot(alpha = .5) +
    geom_point(
      position = position_jitterdodge(dodge.width = .5),
      shape = 21, stroke = .1, size = 2
    ) +
    scale_x_continuous(
      # guide = guide_axis(position = "top"), 
      limits = c(0, 100)
    ) +
    scale_y_discrete(labels = learner_label) +
    scale_fill_manual(values = learner_cols, guide = "none") +
    labs(
      #title = "Aggregated scores over all tasks",
      x = "RRSE (%) (lower is better)", y = NULL
    ) +
    theme_minimal(base_size = 20) +
    theme(
      plot.title.position = "plot"
    )
```

$\mathrm{RRSE} := \sqrt{\mathrm{SSE}(Y, \hat{Y})\ /\ \mathrm{SSE}(Y, \bar{Y})}$

:::

<!--

## Benchmark Results (Selected Tasks)

```{r bm-scores-sel}
#| out-height: "75%"
#| fig-height: 3
#| fig-width: 9
# out-width: "70%"

scores |>
    dplyr::filter(task_id %in% c(
      "space_ga", 
      #"kin8nm", 
      #"white_wine",
      "miami_housing" 
    )) |>
    dplyr::filter(learner_id != "featureless") |>
    dplyr::mutate(
      task_label = stringr::str_replace(task_label, "\\s\\(", "\\\n("),
      task_label = reorder(task_label, n*p)
      #task_label = stringr::str_wrap(task_label, width = 20, whitespace_only = FALSE)
    ) |>
    ggplot(aes(y = learner_id, x = 100 * rrse, fill = learner_id, color = learner_id)) +
    facet_wrap(vars(task_label), ncol = 2, scales = "free") +
    geom_boxplot(alpha = .5) +
    geom_jitter(shape = 21, stroke = .5, alpha = .15, size = 1) +
    scale_fill_manual(values = learner_cols, guide = "none", aesthetics = c("color", "fill")) +
    scale_y_discrete(labels = learner_label) +
    labs(
      # title = "Scores per task",
      # subtitle = "Tasks ordered by n * p, decreasing",
      y = NULL, x = "RRSE (%) (lower is better)"
    ) +
    theme_minimal(base_size = 16) +
    theme(
      panel.spacing.x = unit(1, "cm"),
      plot.title.position = "plot"
    )
```

-->

## Summary

**Random Planted Forests** = Additive Random Forests

\vfill

- (↑) Nicely interpretable feature _importance_ on same scale as target
- (↑) Quantifies _main effects_ and _interaction terms_
- (↑) R package available [^rpfgh]
- (→) Competetive predictive performance (mostly)
- (↓) Computationally heavy for large data (Optimization WIP!)


[^rpfgh]: [github.com/PlantedML/randomPlantedForest](https://github.com/PlantedML/randomPlantedForest)


<!--

Related work: `{glex}`[^glex]

- ANOVA decomposition _post-hoc_ for tree-based methods (e.g. XGBoost)
- **Idea**: 
  - Fit XGBoost (with `max_depth` restricted)
  - Extract component-wise predictions from tree structure
- **Benefit**: Use existing / well known / well optimzied algorithm
- **Drawback**: Computationally intensive post-hoc computation

[^glex]: @hiabu2023glex → [github.com/PlantedML/glex](https://github.com/PlantedML/glex)

-->
