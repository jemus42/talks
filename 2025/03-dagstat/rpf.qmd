---
title: |
  Random Planted Forest\
  A Directly Interpretable Tree Ensemble
subtitle: "Awwww yeah"
author:
  - Meyer, J. T.\inst{5}
  - Burk, L.\inst{1,2,3,4}
  - Hiabu, M.\inst{6}
  - Mammen, E.\inst{5}
#date: "2024-03-01"
format:
  beamer:
    template: template.tex
    widescreen: true # changes to sty now make this the only non-messed up option
    occasion: "DAGStat 2025 --- March 27th, 2025"
    email: burk@leibniz-bips.de
    thankstext: "Slides are online:"
    contactauthor: Lukas Burk
    institute:
      # Can't use \inst because title frame is made with tikz and \inst doesn't work within tikz nodes
      # and \insertinstitute did not work for some other godforsaken reason, idunno
        - \textsuperscript{1}Leibniz Institute for Prevention Research and Epidemiology -- BIPS
        - \textsuperscript{2}LMU Munich \quad \textsuperscript{3}University of Bremen
        - \textsuperscript{4}Munich Center for Machine Learning (MCML)
        - \textsuperscript{5}Heidelberg University \quad \textsuperscript{6}University of Copenhagen
    bibliography:
      - references.bib
    cite-method: biblatex
    keep-tex: true
preview:
  port: 7777
editor_options: 
  chunk_output_type: console
# pdf-engine: xelatex
# bibliography: references.bib
---

```{r setup, include=FALSE}
library(ggplot2)
library(patchwork)
library(randomPlantedForest)
library(glex)
library(kableExtra)

knitr::opts_chunk$set(
  fig.align = "center",
  dev = "ragg_png",
  dev.args = list(bg = "transparent")
)

theme_set(
  theme_minimal(base_size = 20) +
    theme(
      legend.position = "bottom",
      panel.spacing = unit(10, "mm"),
      plot.title.position = "plot"
    )
)

if (FALSE) {
  set.seed(2025)
  rpfit <- rpf(
    bikers ~ hr + temp + workingday + season + weathersit + windspeed + hum,
    data = bike,
    max_interaction = 3,
    ntrees = 50,
    splits = 100,
    t_try = 0.9,
    split_try = 5,
    nthreads = 10
  )
  rpglex <- glex(rpfit, bike)
  saveRDS(rpglex, here::here("2025/03-dagstat/rpglex.rds")) 

  predict(rpfit, bike) |>
    cbind(bikers = bike$bikers) |>
    dplyr::mutate(se = (.pred - bikers)^2) |>
    dplyr::summarise(rmse = sqrt(mean(se)))

  data.frame(.pred = predict(lmfit, bike), bikers = bike$bikers) |>
    dplyr::mutate(se = (.pred - bikers)^2) |>
    dplyr::summarise(rmse = sqrt(mean(se)))
}


rpglex <- readRDS(here::here("2025/03-dagstat/rpglex.rds"))
lmfit <- lm(bikers ~ hr + temp + workingday, data = bike)
```

## Motivation

-   Individual decision trees: Easy to interpret
-   Ensembles (Random Forest): Less so
-   Desirable property: Quantification of main- and interaction effects
-   Example: Additive models, e.g. LM, GAM
-   → Need to manually specify interactions in model fit

## Functional ANOVA Expansion

-   Decompose prediction $\hat{m}(\mathbf{x})$ into terms $\hat{m}_S$ with $S \subseteq \{1, \ldots, s\}$
-   Example for $p = 3$ features $x_1, x_2, x_3$ and degree of interactions $d = 3$

\begin{align*}
\hat{m}(\mathbf{x}) & = \underbrace{\hat{m}_{0}}_{\text{Intercept}} \\
& + \underbrace{\hat{m}_1(x_1) + \hat{m}_2(x_2) + \hat{m}_3(x_3)}_{\text{Main effects}} \\
& + \underbrace{\hat{m}_{1,2}(x_1,x_2) + \hat{m}_{1,3}(x_1,x_3) + \hat{m}_{2,3}(x_2,x_3)}_{\text{2nd order interactions}} \\ 
& + \underbrace{\hat{m}_{1,2,3}(x_1,x_2,x_3)}_{\text{3rd order interaction}}
\end{align*}

<!-- 
Note: terms $\hat{m}_S$ are _constant_ for all observations! 
-->



## Random Planted Forest (RPF) Differences to Random Forest (RF)

- Splits some nodes multiple times (non-binary trees)
- Keeps track of features involved in split
- Stop after adjustable number of splits
- Degree of interaction can be constrained

<!--
::::: columns
::: {.column width="50%"}
**RF**

-   Feature to split is selected by optimization
-   Nodes are split _once_ or turn into leaf nodes
-   → Resulting trees are **binary**
-   
:::

::: {.column width="50%"}
**RPF**

-   Node can be split multiple times 
-   → Resulting trees **not binary**
-   Can constrain maximum degree of interaction for nodes
-   → Reduce complexity
:::
:::::

-->

## Trees in Random Forest


![](tree-cart.pdf){fig-align="center" height=75%}

## Planted Trees

![Planted](tree-planted-simple.pdf){#fig-planted fig-align="center" height=75%}

## Planted Trees

![Planted](tree-planted-large.pdf){#fig-planted fig-align="center" height=75%}


<!--

:::: {.columns}

::: {.column width="50%"}
![CART](simple-cart.pdf){#fig-cart fig-align="center" height=75%}

:::

::: {.column width="50%"}
![Planted](simple-planted.pdf){#fig-planted fig-align="center" height=75%}

:::

::::

-->


## Application Example

-   `Bikeshare` regression dataset [^1]
-   Target: `bikers` number of bikers on a given day in 2011/2012
-   Focus on 3 features for example
    -   `hour` of day $\in \{0, 1, \ldots, 23\}$
    -   `temp` normalized temperature $\in [0, 1]$
    -   `workingday` binary \{`workingday`, `no workingday`\}
    <!-- -   `season` categorical\{`winter`, `spring`, `summer`, `fall`\} -->
- Average prediction: $\hat{m}_0 \approx$ `r round(rpglex$intercept, 1)`

[^1]: from [UCI ML repository](https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset)

## Effect Visualizations

:::: {.columns}

::: {.column width="33%"}

```{r main-hr}
#| out-height: "75%"
#| fig-width: 6
plot_main_effect(rpglex, "hr")
```

:::

. . .

::: {.column width="33%"}

```{r main-temp}
#| out-height: "75%"
#| fig-width: 6
plot_main_effect(rpglex, "temp")
```

:::

. . .

::: {.column width="33%"}

```{r main-workingday}
#| out-height: "75%"
#| fig-width: 6
plot_main_effect(rpglex, "workingday") 
```

:::

::::


## Hour $\times$ Working Day: "Rush Hour Effect"

```{r twoway-hr-workingday} 
#| out-height: "75%"

plot_twoway_effects(rpglex, c("hr", "workingday")) + labs(color = NULL)
```

## 2nd Order

:::: {.columns}

::: {.column width="50%"}

```{r}
#| out-height: "75%"

plot_twoway_effects(rpglex, c("hr", "temp"))

```

:::

. . .

::: {.column width="50%"}

```{r}
#| out-height: "75%"

plot_twoway_effects(rpglex, c("temp", "workingday")) + labs(color = NULL)
```

:::

::::


## Interactions 3x

```{r}
#| out-height: "75%"

plot_threeway_effects(rpglex, c("hr", "temp", "workingday"))

```

## Feature Importance

-   Averaging absolute values of term of interest $\hat{m}(\mathbf{x})$
    -   across all observations $i$,
    -   for set of features of interest $S$

$$\mathrm{VI}(x) = \frac{1}{n} \sum_{i=1}^n |\hat{m}_S(\mathbf{x}_i)|$$

-   Unlike RF Feature importance: Scores _per interaction_ effect
-   Importance scores on same scale as prediction


## Feature Importance per Main Term

```{r vi-plot}
#| fig-width: 9
#| fig-height: 5
#| out-height: "75%"
vi_rpf <- glex_vi(rpglex)

keep_vis <- sapply(c("hr", "temp", "workingday"), \(term) {
  glex:::find_term_matches(term, vi_rpf$term)
}) |> unlist() |> unique()


vi_rpf_subset <- vi_rpf[keep_vis, ]

autoplot(vi_rpf_subset[degree == 1]) + labs(title = NULL) + theme(legend.position = "none")

```


## Feature Importance per Interaction Term

```{r vi-plot-thresh}
#| out-height: "75%"
autoplot(vi_rpf_subset, threshold = 3) + 
  labs(title = NULL, caption = "Values < 3 aggregated under 'Remaining terms'")
```

## Feature Importance by Order of Interaction

```{r vi-plot-degree}
#| out-height: "75%"
autoplot(vi_rpf, by_degree = TRUE) + labs(title = NULL, y = "Order of Interaction")
```

## Related work

Glex[^glex]: Same ANOVA decomposition but for XGBoost (and ranger)

- **Idea**: 
  - Fit XGBoost with lower order trees (e.g. `max_depth = 4`)
  - Extract decomposition from tree structure
- **Benefit**: Use existing / well known / well optimzied algorithm
- **Drawback**: Computationally intensive post-hoc computation

[^glex]: @hiabu2023glex

## Free Lunch / Lack Thereof

Better interpretibility →  worse predictive performance?

. . .

-   Benchmark comparing RPF with XGBoost, RF
-   28 datasets from OpenML-CTR23 regression benchmark suite [^ctr23]
-   Algorithms tuned via Bayesian optimization, 200 evaluations
-   Generally XGBoost best, RPF and RF closely behind

[^ctr23]: @fischer2023openmlctr

## Benchmark Results (Aggregated)

- Root-Relative Squared Error (RRSE) $$\sqrt{\frac{SSE(Y, \hat{Y})}{SSE(Y, \bar{Y})}}$$
- Featureless model scores 1, perfect score 0

## Results for Task



## Outlook

-   Main paper yet unplublished


## Links

- RPF R package on GitHub: `PlantedML/randomPlantedForest`
- Glex R package on GitHub: `PlantedML/glex`
- Glex paper: @hiabu2023glex
- Slides
